{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Semantic_PathLoss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Semantic_PathLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "39456ef6-4ae3-4017-a074-e673064963a6"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "6f69511e-e621-4ae3-d864-10b7d4369374"
      },
      "source": [
        "batch_size = 1000\n",
        "\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "print(tr)\n",
        "\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 81. 141. 123. ... 255. 163. 125.]\n",
            "[174, 114, 132, 3, 178, 198, 108, 152, 27, 45, 44, 225, 178, 136, 237, 93, 77, 166, 178, 37, 46, 183, 35, 4, 254, 108, 40, 247, 134, 35, 204, 7, 73, 146, 140, 130, 207, 157, 235, 55, 205, 175, 49, 101, 200, 172, 184, 25, 45, 140, 228, 75, 226, 106, 96, 56, 44, 115, 3, 180, 34, 4, 158, 132, 63, 235, 232, 148, 67, 159, 108, 27, 159, 223, 189, 134, 183, 3, 136, 117, 118, 99, 207, 180, 118, 3, 83, 31, 234, 62, 83, 210, 135, 130, 229, 62, 52, 4, 250, 212, 73, 226, 141, 13, 219, 157, 234, 8, 160, 69, 230, 86, 165, 97, 179, 173, 216, 210, 49, 173, 152, 11, 72, 51, 64, 216, 186, 18, 220, 141, 143, 171, 35, 236, 21, 57, 3, 94, 161, 134, 180, 138, 167, 177, 255, 171, 202, 94, 244, 63, 189, 92, 14, 199, 211, 28, 59, 13, 103, 112, 96, 10, 199, 113, 54, 115, 245, 6, 106, 2, 186, 94, 170, 4, 37, 11, 47, 21, 123, 186, 52, 183, 20, 63, 88, 6, 202, 131, 255, 13, 161, 71, 207, 9, 37, 11, 145, 217, 212, 70, 195, 218, 22, 88, 35, 252, 49, 233, 108, 61, 16, 88, 152, 190, 97, 252, 197, 198, 228, 10, 47, 212, 184, 215, 2, 245, 189, 224, 100, 60, 2, 211, 59, 215, 2, 63, 170, 108, 29, 4, 113, 92, 37, 170, 132, 133, 151, 18, 22, 103, 37, 90, 123, 86, 211, 126, 4, 175, 38, 152, 119, 115, 221, 11, 165, 197, 198, 126, 11, 52, 148, 82, 50, 132, 166, 29, 8, 37, 9, 207, 4, 148, 39, 231, 201, 25, 8, 144, 71, 223, 213, 119, 45, 249, 254, 168, 34, 213, 223, 209, 186, 57, 26, 42, 25, 121, 250, 167, 2, 194, 237, 57, 213, 178, 194, 71, 143, 113, 151, 166, 192, 178, 112, 55, 198, 173, 202, 179, 162, 137, 33, 50, 233, 51, 63, 6, 93, 214, 246, 196, 86, 167, 98, 192, 10, 126, 147, 135, 26, 157, 222, 45, 85, 125, 38, 72, 201, 142, 205, 196, 124, 76, 203, 67, 33, 198, 96, 17, 240, 168, 55, 108, 131, 241, 83, 248, 28, 3, 1, 220, 73, 5, 205, 252, 199, 95, 60, 246, 140, 177, 89, 187, 242, 64, 194, 164, 128, 231, 194, 98, 152, 29, 89, 200, 154, 118, 153, 12, 197, 247, 82, 193, 213, 211, 104, 83, 186, 149, 37, 45, 253, 17, 1, 10, 116, 77, 99, 220, 143, 178, 143, 7, 202, 166, 67, 48, 134, 105, 175, 171, 177, 28, 236, 83, 64, 78, 227, 44, 73, 27, 59, 142, 167, 100, 252, 183, 149, 43, 46, 15, 245, 2, 150, 71, 216, 10, 160, 150, 157, 165, 124, 154, 105, 113, 33, 183, 205, 147, 124, 82, 172, 106, 66, 17, 128, 165, 245, 77, 148, 67, 119, 189, 93, 22, 61, 83, 141, 97, 209, 135, 36, 214, 157, 161, 108, 161, 225, 173, 204, 149, 144, 41, 250, 60, 128, 186, 95, 225, 236, 122, 87, 82, 152, 187, 70, 35, 215, 17, 10, 234, 244, 30, 102, 102, 196, 147, 187, 170, 141, 81, 69, 227, 57, 2, 8, 93, 222, 218, 156, 215, 125, 133, 13, 240, 72, 119, 106, 86, 194, 66, 220, 249, 184, 125, 200, 253, 9, 145, 68, 176, 21, 100, 124, 194, 48, 101, 36, 161, 220, 159, 76, 237, 37, 140, 221, 108, 12, 168, 186, 52, 153, 250, 99, 47, 8, 109, 121, 40, 110, 222, 210, 140, 94, 34, 20, 109, 129, 117, 14, 44, 74, 132, 236, 37, 22, 47, 15, 19, 96, 76, 40, 73, 253, 255, 44, 67, 196, 137, 225, 68, 226, 0, 253, 151, 124, 197, 38, 44, 68, 221, 42, 30, 220, 98, 223, 44, 119, 62, 97, 250, 66, 249, 105, 222, 134, 6, 78, 252, 106, 83, 2, 217, 23, 14, 4, 67, 168, 21, 114, 252, 214, 162, 68, 104, 122, 191, 213, 184, 71, 54, 170, 119, 234, 228, 227, 201, 42, 41, 8, 195, 228, 139, 4, 129, 72, 32, 217, 69, 210, 217, 247, 92, 118, 42, 87, 217, 189, 98, 49, 117, 161, 148, 165, 7, 236, 118, 102, 55, 16, 199, 103, 85, 35, 88, 205, 224, 56, 9, 125, 75, 133, 203, 214, 209, 114, 229, 236, 3, 185, 22, 169, 201, 116, 204, 245, 231, 42, 26, 177, 48, 184, 69, 98, 112, 222, 224, 44, 82, 157, 48, 32, 240, 247, 145, 71, 77, 114, 96, 46, 22, 3, 75, 68, 243, 214, 181, 145, 175, 70, 147, 47, 161, 243, 83, 159, 62, 212, 216, 194, 29, 223, 20, 101, 71, 76, 3, 34, 91, 232, 70, 100, 144, 179, 213, 9, 121, 6, 101, 228, 173, 59, 62, 102, 153, 85, 27, 176, 121, 69, 110, 135, 199, 202, 41, 134, 233, 56, 186, 185, 187, 138, 108, 198, 96, 203, 50, 253, 70, 120, 33, 87, 88, 253, 228, 32, 212, 70, 158, 81, 183, 134, 255, 138, 190, 30, 112, 11, 199, 39, 101, 0, 222, 26, 255, 220, 121, 62, 125, 82, 34, 227, 93, 240, 217, 20, 9, 189, 64, 198, 57, 148, 199, 1, 159, 157, 102, 138, 104, 167, 121, 19, 157, 10, 169, 233, 103, 219, 254, 145, 10, 10, 197, 150, 244, 144, 46, 70, 152, 159, 131, 200, 181, 72, 46, 160, 184, 67, 104, 120, 64, 20, 181, 9, 144, 9, 246, 197, 100, 187, 191, 97, 71, 26, 214, 228, 76, 196, 56, 14, 38, 52, 78, 224, 248, 1, 130, 131, 186, 247, 121, 111, 150, 113, 96, 77, 189, 160, 72, 130, 55, 242, 2, 254, 60, 136, 42, 106, 59, 178, 94, 181, 175, 108, 104, 224, 200, 76, 46, 219, 252, 119, 113, 165, 200, 87, 46, 177, 67, 172, 40, 154, 95, 240, 194, 144, 157, 129, 104, 9, 142, 41, 52, 132, 66, 87, 72, 226, 215, 223, 224, 213, 255, 78, 153, 228, 161, 11, 10, 227, 2, 80, 119, 185, 145, 13, 84, 179, 209, 1, 117, 136, 5, 188, 249, 233, 226, 191, 90, 122, 21, 74, 34, 63, 57, 49, 56, 156, 12, 0, 3, 141, 95, 27, 29, 151, 206, 208, 77, 21, 113, 223, 3, 175, 50, 189, 5, 202, 88, 253, 126, 185, 110, 255, 216, 233, 234, 177, 199, 13, 185, 193, 44, 25, 86, 65, 33, 32, 248, 134, 218, 209, 169, 171, 243, 54, 40, 186, 78, 102, 90, 224, 158, 25, 207, 22, 104, 133, 109, 248, 76, 151, 185, 33, 12, 178, 72, 239, 174, 223, 160, 253, 67, 150, 57, 183, 186, 65, 36, 249, 147, 7, 125, 143, 35, 106, 7, 152, 57, 89, 62, 31, 144, 70, 59, 11, 102, 43, 16, 49, 153, 36, 6, 251, 210, 56, 109, 147, 223, 31, 13, 206, 118, 244, 186, 236, 251, 173, 128, 239, 156, 129, 148, 115, 50, 40, 101, 162, 253, 251, 240, 226, 10, 210, 111, 79, 232, 245, 200, 76, 226, 117, 132, 129, 232, 75, 241, 241, 177, 23, 175, 143, 85, 203, 252, 215, 110, 215, 243, 119, 19, 8, 92, 226, 154, 20, 170, 233, 217, 229, 245, 37, 172, 180, 149, 240, 10, 240, 116, 91, 2, 241, 186, 61, 220, 90, 83, 70, 210, 133, 197, 217, 180, 82, 69, 11, 30, 8, 67, 194, 102, 162, 171, 11, 61, 22, 247, 2, 76, 49, 52, 120, 117, 116, 201, 130, 79, 214, 111, 35, 100, 54, 14, 206, 81, 75, 15, 194, 142, 216, 28, 124, 54, 78, 204, 19, 179, 124, 46, 96, 128, 74, 251, 181, 216, 37, 230, 214, 10, 14, 102, 188, 91, 66, 59, 222, 33, 232, 99, 168, 184, 114, 122, 33, 28, 102, 71, 181, 100, 5, 243, 4, 30, 135, 80, 44, 82, 143, 239, 245, 98, 3, 203, 116, 185, 225, 125, 76, 139, 172, 134, 222, 120, 111, 173, 84, 32, 30, 112, 250, 65, 61, 40, 94, 30, 143, 142, 221, 250, 162, 35, 13, 57, 70, 246, 189, 141, 103, 18, 84, 85, 204, 2, 158, 18, 97, 243, 13, 221, 129, 1, 144, 91, 75, 252, 188, 234, 50, 82, 91, 62, 47, 114, 6, 230, 64, 143, 83, 24, 28, 35, 226, 221, 11, 115, 109, 123, 136, 114, 175, 43, 135, 153, 166, 121, 221, 179, 142, 130, 217, 14, 63, 158, 63, 107, 63, 171, 93, 85, 143, 41, 231, 199, 231, 255, 215, 163, 179, 127, 152, 61, 208, 130, 160, 206, 227, 48, 12, 135, 55, 120, 202, 184, 202, 28, 124, 142, 190, 174, 128, 220, 9, 76, 55, 227, 101, 233, 31, 193, 118, 153, 45, 42, 16, 10, 182, 104, 234, 198, 255, 186, 18, 190, 240, 41, 212, 93, 194, 220, 210, 210, 224, 99, 154, 109, 183, 68, 152, 19, 110, 223, 198, 136, 44, 54, 219, 87, 223, 11, 24, 206, 231, 39, 242, 251, 57, 59, 154, 138, 152, 160, 164, 65, 116, 109, 165, 189, 219, 25, 90, 216, 104, 169, 67, 54, 156, 74, 127, 38, 8, 243, 163, 179, 41, 14, 23, 160, 243, 40, 216, 207, 227, 135, 52, 42, 170, 60, 63, 121, 9, 190, 159, 13, 232, 127, 33, 80, 64, 131, 211, 138, 30, 11, 196, 90, 113, 246, 26, 32, 133, 47, 73, 247, 20, 2, 240, 96, 145, 153, 164, 123, 254, 176, 202, 56, 3, 160, 226, 60, 4, 4, 158, 83, 160, 162, 20, 231, 26, 69, 179, 92, 98, 92, 75, 53, 134, 85, 34, 89, 156, 51, 196, 135, 42, 99, 69, 18, 120, 253, 8, 196, 23, 238, 86, 137, 30, 222, 250, 162, 54, 7, 248, 229, 113, 72, 230, 164, 181, 191, 231, 188, 192, 67, 128, 43, 197, 95, 32, 176, 244, 63, 210, 127, 63, 196, 123, 10, 238, 4, 29, 28, 91, 199, 208, 36, 7, 70, 12, 50, 34, 162, 26, 155, 207, 55, 144, 225, 5, 5, 209, 35, 208, 41, 47, 142, 170, 48, 60, 67, 242, 24, 101, 25, 16, 28, 45, 199, 159, 238, 241, 26, 195, 101, 157, 107, 234, 146, 148, 248, 95, 200, 97, 167, 138, 161, 31, 2, 78, 93, 202, 0, 202, 15, 223, 219, 74, 77, 255, 62, 100, 23, 54, 113, 70, 137, 62, 212, 48, 211, 109, 27, 47, 10, 33, 116, 193, 62, 249, 75, 223, 102, 178, 182, 5, 84, 59, 22, 119, 183, 58, 152, 148, 90, 36, 162, 184, 59, 219, 124, 95, 29, 69, 99, 137, 4, 33, 189, 197, 151, 154, 132, 253, 198, 176, 233, 141, 235, 253, 166, 66, 45, 194, 78, 100, 39, 62, 138, 44, 107, 200, 188, 15, 13, 210, 221, 154, 109, 248, 179, 111, 24, 231, 161, 68, 116, 223, 67, 84, 29, 13, 246, 93, 242, 146, 99, 11, 138, 74, 18, 63, 203, 58, 2, 63, 242, 211, 232, 148, 95, 202, 50, 152, 25, 184, 50, 12, 139, 3, 10, 145, 196, 129, 72, 109, 108, 158, 159, 36, 58, 180, 11, 0, 24, 228, 97, 87, 207, 241, 124, 114, 75, 232, 11, 173, 229, 220, 66, 174, 202, 191, 62, 140, 164, 145, 122, 67, 218, 149, 104, 94, 6, 55, 240, 52, 65, 128, 153, 158, 237, 27, 178, 162, 217, 175, 255, 83, 126, 84, 28, 171, 47, 100, 144, 177, 46, 178, 63, 79, 41, 205, 251, 91, 190, 128, 219, 140, 17, 119, 172, 216, 105, 201, 247, 172, 176, 197, 3, 138, 132, 176, 53, 201, 240, 144, 142, 11, 206, 124, 229, 53, 57, 10, 195, 137, 125, 137, 66, 188, 131, 110, 168, 227, 213, 184, 86, 201, 93, 113, 210, 23, 151, 75, 145, 98, 83, 242, 116, 93, 168, 253, 134, 96, 243, 17, 118, 89, 228, 234, 154, 135, 231, 54, 33, 102, 169, 99, 43, 244, 37, 65, 250, 132, 122, 155, 80, 237, 165, 237, 80, 220, 76, 184, 7, 32, 184, 40, 70, 205, 154, 33, 243, 9, 0, 159, 78, 19, 113, 235, 237, 221, 142, 145, 48, 82, 208, 62, 153, 28, 75, 217, 168, 33, 64, 167, 255, 21, 15, 124, 210, 233, 88, 180, 8, 109, 158, 30, 176, 216, 22, 154, 230, 175, 111, 120, 69, 50, 164, 183, 218, 104, 203, 59, 7, 121, 219, 54, 45, 69, 121, 159, 129, 24, 124, 246, 116, 38, 245, 48, 245, 153, 248, 11, 176, 193, 90, 88, 25, 155, 157, 222, 29, 206, 24, 228, 34, 143, 68, 135, 243, 147, 0, 113, 187, 143, 253, 239, 241, 212, 146, 77, 149, 187, 143, 183, 12, 225, 251, 243, 209, 81, 41, 186, 175, 112, 30, 195, 217, 93, 6, 104, 213, 166, 100, 215, 113, 94, 241, 123, 96, 32, 39, 247, 87, 92, 28, 81, 115, 80, 165, 226, 146, 45, 1, 223, 199, 199, 103, 175, 114, 231, 215, 61, 196, 160, 210, 145, 176, 143, 203, 47, 40, 244, 214, 148, 43, 136, 48, 9, 105, 29, 65, 107, 88, 30, 227, 242, 207, 236, 225, 18, 199, 118, 164, 109, 15, 150, 34, 14, 54, 11, 39, 157, 151, 88, 93, 227, 35, 94, 221, 57, 100, 199, 110, 91, 19, 128, 138, 74, 248, 147, 38, 117, 153, 246, 97, 17, 28, 104, 162, 210, 154, 39, 167, 238, 71, 185, 120, 88, 206, 237, 96, 167, 67, 67, 82, 27, 155, 92, 91, 154, 241, 61, 24, 149, 139, 44, 6, 84, 246, 103, 211, 218, 124, 186, 29, 248, 100, 163, 174, 94, 224, 117, 156, 98, 236, 233, 36, 224, 12, 212, 196, 156, 64, 197, 151, 48, 227, 118, 253, 67, 86, 216, 177, 102, 74, 148, 110, 35, 79, 228, 29, 230, 20, 162, 120, 69, 21, 5, 81, 169, 135, 21, 179, 177, 186, 37, 247, 167, 93, 126, 131, 132, 155, 119, 147, 33, 111, 11, 41, 40, 73, 11, 193, 31, 161, 185, 48, 78, 28, 240, 33, 134, 232, 108, 87, 206, 235, 35, 32, 203, 29, 83, 22, 91, 81, 54, 105, 152, 102, 184, 190, 1, 86, 240, 165, 53, 191, 207, 255, 224, 170, 189, 116, 42, 116, 170, 151, 227, 4, 106, 1, 181, 112, 64, 15, 68, 187, 104, 122, 80, 26, 69, 134, 60, 228, 255, 80, 67, 234, 10, 137, 128, 3, 232, 156, 186, 228, 242, 144, 15, 10, 31, 24, 30, 67, 129, 237, 24, 168, 58, 112, 135, 221, 50, 147, 140, 149, 192, 52, 153, 228, 68, 169, 213, 172, 247, 156, 27, 243, 88, 34, 244, 175, 219, 47, 6, 194, 223, 95, 226, 101, 29, 98, 65, 156, 71, 110, 253, 61, 235, 61, 142, 255, 238, 200, 130, 204, 253, 66, 208, 44, 92, 185, 127, 137, 125, 131, 191, 215, 108, 216, 232, 122, 179, 125, 73, 9, 28, 14, 142, 244, 185, 34, 43, 165, 136, 78, 117, 62, 32, 28, 150, 31, 249, 218, 232, 246, 135, 13, 85, 14, 180, 253, 250, 169, 108, 81, 165, 96, 124, 136, 248, 44, 217, 67, 129, 254, 97, 200, 123, 141, 255, 103, 163, 50, 179, 38, 77, 18, 201, 236, 134, 163, 73, 8, 246, 254, 51, 89, 211, 183, 211, 117, 69, 154, 109, 149, 34, 88, 225, 96, 198, 86, 112, 117, 119, 96, 40, 37, 92, 17, 207, 194, 61, 102, 69, 118, 54, 64, 229, 124, 234, 220, 189, 121, 33, 153, 117, 137, 170, 74, 1, 36, 19, 179, 59, 81, 188, 67, 182, 54, 215, 60, 113, 239, 107, 217, 206, 216, 142, 247, 8, 108, 223, 86, 224, 39, 185, 37, 223, 252, 133, 219, 96, 149, 187, 245, 174, 217, 222, 148, 119, 11, 57, 67, 138, 210, 70, 145, 253, 11, 135, 158, 24, 136, 96, 227, 137, 129, 134, 32, 180, 150, 88, 199, 222, 255, 180, 170, 59, 236, 14, 187, 18, 197, 83, 98, 91, 86, 255, 197, 161, 233, 132, 101, 249, 131, 117, 198, 86, 27, 150, 214, 82, 248, 235, 202, 225, 51, 189, 77, 141, 235, 208, 104, 55, 50, 214, 0, 246, 35, 89, 150, 102, 192, 24, 106, 45, 2, 53, 44, 36, 55, 22, 99, 58, 209, 161, 83, 173, 198, 200, 225, 172, 114, 69, 98, 72, 118, 220, 6, 52, 151, 184, 70, 114, 151, 120, 250, 100, 59, 41, 36, 147, 161, 69, 217, 63, 240, 29, 1, 173, 32, 137, 79, 137, 22, 122, 250, 224, 247, 164, 200, 68, 168, 84, 83, 50, 41, 177, 195, 130, 178, 233, 228, 233, 184, 61, 205, 128, 228, 189, 98, 223, 249, 149, 123, 54, 224, 206, 66, 70, 167, 0, 218, 246, 210, 155, 214, 98, 45, 61, 202, 139, 170, 149, 85, 45, 67, 176, 207, 101, 231, 198, 152, 228, 70, 100, 122, 75, 121, 233, 130, 5, 148, 67, 230, 135, 135, 207, 77, 100, 188, 35, 120, 50, 91, 1, 146, 210, 141, 239, 58, 31, 205, 21, 57, 213, 90, 94, 29, 111, 200, 255, 228, 185, 58, 203, 43, 231, 60, 198, 156, 157, 246, 92, 168, 113, 215, 149, 70, 75, 142, 6, 10, 32, 53, 82, 189, 218, 34, 166, 177, 181, 61, 74, 158, 182, 125, 11, 169, 214, 79, 52, 235, 217, 244, 212, 19, 4, 51, 154, 97, 206, 89, 122, 151, 101, 61, 101, 126, 125, 40, 66, 254, 47, 11, 227, 25, 40, 160, 146, 97, 122, 73, 34, 87, 114, 142, 76, 165, 33, 201, 174, 158, 238, 219, 0, 43, 122, 161, 141, 170, 114, 239, 105, 123, 248, 103, 157, 34, 101, 243, 125, 144, 244, 219, 18, 207, 112, 82, 41, 51, 82, 72, 126, 97, 40, 103, 80, 229, 203, 89, 104, 217, 173, 6, 222, 204, 31, 157, 62, 207, 168, 75, 151, 122, 85, 143, 246, 28, 59, 227, 135, 253, 121, 64, 125, 186, 64, 13, 2, 175, 204, 108, 139, 76, 251, 49, 214, 18, 9, 90, 161, 95, 211, 18, 228, 234, 83, 32, 162, 151, 109, 110, 127, 111, 89, 153, 160, 116, 116, 64, 116, 254, 156, 107, 41, 145, 211, 75, 246, 48, 78, 194, 253, 88, 195, 43, 30, 240, 101, 12, 11, 234, 173, 56, 90, 217, 54, 133, 20, 199, 95, 113, 122, 20, 193, 76, 141, 41, 143, 115, 88, 230, 152, 115, 7, 228, 238, 34, 254, 50, 62, 132, 251, 167, 98, 103, 47, 38, 158, 244, 10, 80, 51, 161, 120, 55, 211, 39, 199, 104, 246, 137, 34, 156, 172, 196, 170, 122, 118, 250, 53, 8, 146, 241, 72, 37, 26, 233, 54, 86, 159, 172, 64, 71, 63, 80, 16, 55, 187, 39, 221, 213, 167, 55, 84, 255, 0, 67, 221, 20, 182, 148, 119, 55, 118, 244, 113, 13, 233, 168, 191, 24, 145, 129, 128, 114, 210, 238, 52, 231, 103, 185, 30, 24, 78, 233, 208, 23, 127, 208, 250, 216, 116, 219, 173, 46, 9, 73, 180, 192, 195, 195, 128, 189, 20, 88, 215, 44, 160, 73, 196, 201, 16, 188, 203, 172, 96, 192, 41, 44, 134, 234, 103, 10, 240, 73, 147, 163, 195, 61, 112, 118, 212, 77, 196, 248, 159, 209, 109, 125, 177, 176, 120, 135, 15, 162, 74, 51, 14, 87, 212, 85, 186, 253, 78, 207, 118, 206, 203, 223, 193, 7, 57, 194, 186, 19, 111, 196, 187, 79, 26, 207, 17, 219, 30, 84, 195, 183, 239, 178, 104, 123, 217, 27, 172, 39, 223, 233, 128, 184, 160, 185, 215, 212, 54, 227, 0, 216, 91, 129, 12, 94, 6, 208, 133, 162, 5, 36, 86, 89, 103, 111, 77, 78, 194, 161, 149, 153, 176, 176, 229, 5, 250, 215, 25, 16, 190, 237, 149, 98, 1, 242, 197, 141, 117, 142, 213, 140, 67, 104, 178, 226, 244, 179, 202, 215, 218, 43, 12, 203, 162, 56, 18, 51, 173, 246, 126, 139, 235, 76, 62, 251, 215, 96, 82, 19, 48, 53, 96, 147, 0, 77, 93, 121, 27, 187, 152, 198, 195, 246, 251, 95, 235, 179, 92, 63, 114, 75, 100, 223, 253, 163, 212, 135, 26, 28, 49, 123, 188, 123, 117, 185, 43, 232, 232, 173, 113, 96, 77, 82, 231, 234, 78, 100, 173, 208, 202, 125, 93, 99, 138, 99, 156, 60, 179, 37, 100, 18, 46, 92, 129, 111, 45, 220, 18, 16, 5, 162, 129, 111, 245, 232, 179, 207, 103, 129, 32, 253, 186, 224, 88, 50, 238, 19, 210, 200, 113, 133, 222, 140, 96, 57, 150, 214, 107, 246, 82, 99, 141, 89, 171, 229, 142, 213, 120, 104, 179, 13, 108, 186, 153, 169, 132, 31, 147, 46, 192, 26, 73, 170, 175, 85, 101, 177, 175, 229, 94, 78, 101, 146, 125, 36, 34, 67, 199, 205, 23, 79, 230, 252, 109, 2, 155, 151, 117, 58, 48, 21, 65, 155, 45, 138, 250, 125, 222, 131, 202, 73, 51, 52, 81, 198, 67, 41, 123, 77, 77, 12, 253, 225, 7, 11, 148, 233, 35, 222, 193, 189, 61, 205, 251, 180, 110, 169, 54, 244, 216, 52, 2, 207, 174, 113, 238, 89, 213, 155, 227, 15, 58, 95, 90, 161, 125, 126, 119, 193, 5, 87, 179, 130, 197, 124, 113, 198, 235, 162, 249, 166, 135, 8, 50, 200, 25, 141, 182, 122, 244, 254, 172, 146, 195, 44, 194, 135, 210, 204, 62, 76, 181, 48, 43, 38, 238, 106, 223, 120, 148, 45, 189, 115, 235, 55, 160, 16, 71, 225, 31, 208, 215, 34, 121, 249, 127, 57, 134, 144, 144, 37, 168, 187, 234, 18, 100, 193, 234, 186, 186, 174, 8, 138, 164, 109, 213, 170, 13, 56, 217, 76, 162, 52, 80, 177, 158, 115, 19, 180, 126, 65, 108, 36, 147, 78, 6, 112, 66, 196, 246, 48, 172, 47, 7, 180, 59, 159, 152, 10, 141, 90, 10, 176, 200, 207, 175, 63, 186, 66, 136, 28, 150, 179, 120, 103, 142, 194, 18, 232, 145, 101, 30, 24, 102, 7, 181, 209, 165, 87, 31, 222, 207, 96, 192, 124, 243, 24, 145, 104, 175, 64, 95, 190, 253, 24, 46, 69, 48, 76, 230, 238, 170, 185, 227, 4, 235, 118, 230, 36, 17, 104, 27, 154, 241, 206, 8, 132, 182, 13, 191, 227, 187, 168, 53, 198, 216, 164, 157, 121, 241, 168, 62, 123, 31, 80, 206, 239, 208, 207, 79, 174, 70, 62, 92, 144, 88, 230, 237, 249, 85, 67, 114, 70, 99, 157, 79, 121, 150, 138, 92, 247, 128, 71, 81, 38, 70, 167, 126, 0, 185, 174, 37, 79, 151, 11, 224, 38, 248, 227, 95, 182, 76, 99, 74, 18, 240, 136, 168, 233, 250, 107, 116, 204, 2, 205, 222, 227, 52, 190, 227, 77, 26, 242, 59, 168, 137, 191, 79, 29, 27, 199, 153, 94, 243, 8, 50, 136, 109, 81, 122, 10, 12, 98, 203, 46, 196, 39, 139, 210, 88, 154, 31, 55, 82, 183, 181, 169, 142, 29, 26, 238, 170, 167, 42, 124, 207, 241, 12, 47, 46, 50, 52, 7, 114, 164, 143, 192, 215, 226, 200, 141, 13, 215, 241, 153, 235, 49, 42, 143, 191, 63, 217, 220, 191, 24, 207, 185, 36, 126, 126, 93, 152, 52, 238, 230, 5, 188, 43, 236, 108, 219, 53, 61, 104, 187, 116, 44, 237, 62, 246, 197, 37, 169, 13, 186, 2, 214, 232, 98, 215, 63, 47, 26, 44, 100, 109, 203, 209, 177, 187, 246, 184, 67, 94, 132, 25, 173, 234, 21, 125, 90, 22, 65, 181, 149, 15, 140, 5, 128, 209, 149, 177, 158, 183, 187, 11, 123, 244, 56, 144, 121, 225, 145, 196, 144, 48, 126, 120, 154, 161, 207, 18, 132, 168, 150, 201, 33, 32, 114, 185, 194, 233, 233, 143, 100, 197, 51, 22, 193, 10, 135, 144, 143, 150, 119, 14, 25, 228, 126, 200, 25, 197, 191, 211, 17, 43, 34, 247, 44, 180, 4, 141, 122, 209, 248, 80, 72, 214, 204, 188, 127, 62, 155, 41, 93, 71, 125, 58, 13, 157, 165, 107, 118, 180, 238, 43, 214, 139, 119, 28, 244, 204, 76, 31, 121, 182, 223, 42, 113, 162, 149, 106, 98, 113, 89, 147, 218, 138, 239, 153, 70, 57, 195, 215, 34, 18, 146, 75, 29, 118, 207, 171, 131, 104, 98, 17, 137, 27, 164, 235, 248, 46, 28, 25, 137, 5, 32, 199, 201, 153, 18, 81, 17, 96, 141, 88, 110, 16, 206, 197, 250, 210, 97, 37, 130, 136, 67, 17, 136, 158, 162, 36, 177, 11, 177, 30, 94, 32, 206, 252, 61, 231, 253, 160, 100, 84, 245, 147, 167, 96, 67, 132, 178, 20, 104, 72, 97, 32, 219, 130, 49, 153, 91, 12, 207, 9, 132, 193, 58, 222, 226, 157, 57, 138, 173, 107, 68, 207, 113, 158, 124, 192, 39, 128, 254, 106, 242, 203, 215, 176, 121, 5, 148, 140, 124, 138, 162, 179, 243, 244, 157, 243, 227, 187, 109, 30, 9, 170, 208, 89, 9, 231, 164, 93, 163, 136, 98, 168, 150, 253, 67, 235, 15, 152, 231, 27, 44, 255, 204, 4, 140, 2, 58, 194, 182, 157, 163, 241, 111, 188, 68, 24, 195, 238, 11, 165, 111, 253, 99, 203, 110, 85, 96, 155, 134, 59, 109, 242, 76, 179, 59, 116, 165, 7, 159, 75, 77, 24, 182, 56, 134, 211, 197, 17, 0, 217, 137, 161, 238, 147, 144, 220, 106, 36, 56, 119, 180, 225, 114, 39, 166, 205, 28, 97, 116, 249, 22, 206, 139, 58, 255, 148, 112, 178, 93, 99, 172, 130, 45, 155, 162, 199, 128, 101, 186, 227, 105, 193, 47, 88, 229, 185, 95, 43, 118, 150, 111, 80, 117, 167, 87, 202, 127, 148, 2, 197, 37, 201, 120, 17, 171, 40, 174, 119, 18, 156, 121, 135, 243, 131, 33, 155, 228, 247, 229, 62, 36, 14, 253, 19, 186, 233, 36, 5, 76, 66, 93, 120, 122, 96, 239, 56, 13, 127, 176, 165, 97, 218, 25, 147, 111, 154, 178, 151, 41, 194, 147, 137, 126, 46, 102, 85, 207, 143, 252, 101, 72, 22, 0, 245, 67, 252, 172, 49, 38, 160, 237, 162, 159, 217, 227, 131, 96, 253, 193, 212, 252, 218, 169, 209, 244, 184, 53, 86, 183, 116, 217, 172, 146, 31, 85, 186, 112, 101, 197, 11, 198, 97, 77, 88, 19, 188, 90, 134, 85, 185, 91, 255, 128, 111, 37, 219, 235, 48, 86, 59, 231, 148, 83, 66, 62, 167, 18, 127, 177, 122, 146, 214, 59, 4, 26, 117, 211, 142, 167, 46, 74, 141, 71, 206, 31, 4, 104, 71, 32, 117, 232, 205, 66, 108, 102, 30, 255, 22, 145, 221, 232, 89, 103, 121, 154, 61, 31, 88, 183, 248, 91, 49, 175, 127, 165, 219, 122, 225, 162, 119, 13, 248, 202, 151, 137, 186, 212, 102, 137, 167, 7, 137, 2, 110, 141, 138, 21, 77, 217, 76, 134, 226, 70, 56, 67, 27, 212, 33, 95, 155, 78, 135, 177, 234, 27, 255, 33, 230, 51, 180, 146, 58, 72, 199, 82, 247, 240, 89, 188, 195, 239, 216, 194, 18, 186, 177, 196, 4, 221, 212, 119, 105, 75, 170, 221, 92, 6, 220, 87, 198, 98, 132, 199, 24, 226, 249, 186, 190, 218, 95, 86, 207, 119, 14, 22, 44, 179, 35, 173, 26, 236, 3, 187, 48, 5, 94, 119, 129, 160, 175, 15, 210, 175, 214, 1, 182, 137, 25, 180, 162, 11, 210, 251, 32, 38, 174, 116, 93, 13, 130, 178, 44, 140, 84, 88, 46, 122, 28, 80, 81, 53, 29, 59, 243, 140, 235, 227, 94, 45, 62, 86, 10, 40, 81, 5, 115, 54, 83, 220, 42, 189, 138, 21, 246, 146, 12, 76, 198, 142, 167, 69, 204, 204, 226, 54, 153, 205, 131, 224, 120, 147, 82, 141, 11, 73, 80, 198, 182, 0, 108, 140, 47, 154, 186, 192, 97, 144, 33, 41, 151, 47, 9, 103, 171, 223, 105, 240, 239, 115, 183, 136, 117, 77, 26, 120, 248, 251, 22, 139, 144, 213, 4, 206, 27, 87, 37, 114, 120, 169, 110, 175, 62, 77, 45, 29, 95, 34, 206, 63, 134, 201, 162, 46, 224, 211, 150, 182, 24, 162, 21, 200, 207, 31, 116, 62, 53, 58, 132, 96, 211, 31, 87, 60, 27, 62, 34, 83, 92, 212, 39, 9, 250, 245, 8, 92, 22, 80, 190, 192, 160, 219, 79, 56, 117, 82, 51, 41, 136, 206, 62, 253, 181, 252, 7, 252, 155, 250, 60, 196, 23, 147, 214, 221, 144, 226, 58, 201, 178, 71, 201, 176, 252, 4, 83, 173, 247, 129, 89, 154, 66, 7, 161, 234, 98, 89, 81, 211, 133, 90, 190, 164, 137, 81, 195, 71, 51, 192, 34, 64, 61, 87, 134, 153, 82, 16, 134, 20, 156, 242, 216, 194, 217, 5, 71, 160, 121, 188, 165, 28, 136, 49, 85, 96, 120, 63, 39, 139, 140, 229, 235, 202, 63, 136, 10, 42, 135, 180, 165, 144, 20, 246, 55, 27, 7, 224, 208, 127, 76, 195, 96, 40, 161, 130, 76, 149, 216, 221, 230, 89, 182, 149, 205, 39, 204, 25, 57, 49, 203, 43, 165, 133, 32, 107, 223, 7, 113, 89, 50, 219, 97, 48, 189, 188, 159, 139, 57, 103, 162, 212, 93, 109, 84, 23, 29, 201, 204, 180, 82, 213, 3, 7, 24, 125, 253, 103, 206, 170, 138, 140, 187, 249, 16, 156, 163, 24, 196, 140, 81, 245, 248, 120, 248, 113, 61, 89, 211, 64, 30, 23, 129, 118, 208, 229, 24, 13, 71, 20, 182, 212, 66, 6, 81, 20, 237, 183, 89, 164, 63, 91, 138, 175, 70, 74, 255, 30, 33, 135, 249, 30, 144, 52, 97, 135, 255, 13, 131, 174, 42, 176, 104, 7, 232, 89, 157, 10, 254, 223, 202, 144, 159, 194, 27, 241, 41, 135, 126, 177, 251, 159, 243, 234, 129, 195, 139, 56, 226, 220, 226, 125, 104, 184, 101, 151, 206, 88, 110, 213, 14, 25, 220, 78, 211, 72, 193, 128, 13, 144, 0, 92, 187, 81, 68, 95, 145, 64, 34, 205, 23, 138, 83, 220, 91, 91, 227, 208, 214, 40, 168, 243, 47, 75, 149, 219, 195, 111, 72, 198, 117, 166, 114, 9, 12, 185, 220, 42, 166, 6, 169, 79, 189, 174, 6, 229, 243, 93, 194, 150, 194, 153, 22, 161, 33, 220, 94, 177, 67, 63, 89, 85, 226, 250, 155, 83, 23, 25, 156, 78, 216, 206, 104, 224, 219, 6, 29, 89, 14, 120, 102, 188, 107, 94, 132, 86, 23, 175, 8, 120, 107, 6, 154, 96, 16, 250, 115, 244, 244, 147, 172, 42, 23, 122, 231, 34, 69, 50, 129, 15, 137, 200, 68, 45, 144, 132, 229, 13, 68, 102, 64, 108, 204, 168, 178, 157, 37, 102, 81, 240, 66, 79, 17, 253, 231, 138, 101, 195, 153, 87, 181, 33, 49, 107, 136, 95, 97, 116, 21, 29, 209, 0, 50, 98, 55, 74, 11, 234, 219, 60, 58, 168, 252, 255, 254, 227, 150, 251, 51, 192, 98, 31, 176, 3, 54, 184, 157, 103, 254, 20, 142, 237, 61, 110, 99, 14, 37, 16, 207, 255, 102, 87, 73, 40, 26, 80, 11, 72, 168, 163, 214, 55, 209, 166, 45, 34, 232, 74, 100, 165, 196, 236, 187, 25, 230, 94, 220, 32, 164, 104, 193, 127, 222, 144, 94, 63, 159, 97, 228, 25, 223, 16, 35, 28, 239, 253, 135, 103, 86, 212, 53, 248, 174, 65, 238, 109, 238, 10, 24, 23, 11, 231, 178, 226, 119, 231, 67, 145, 135, 85, 118, 230, 36, 240, 153, 90, 74, 41, 30, 184, 175, 13, 144, 54, 28, 148, 116, 50, 223, 129, 156, 60, 83, 149, 85, 187, 173, 28, 86, 1, 240, 136, 203, 178, 235, 94, 201, 142, 96, 198, 30, 235, 251, 192, 88, 108, 93, 3, 93, 144, 44, 181, 141, 243, 8, 217, 85, 235, 37, 128, 6, 53, 79, 200, 210, 232, 87, 137, 158, 57, 2, 176, 138, 54, 233, 159, 35, 244, 102, 252, 211, 185, 7, 159, 83, 188, 119, 30, 177, 81, 217, 182, 173, 75, 2, 221, 162, 204, 103, 53, 220, 194, 171, 87, 42, 37, 213, 219, 84, 21, 77, 162, 119, 114, 24, 69, 54, 218, 253, 44, 205, 4, 52, 185, 42, 129, 230, 5, 194, 13, 240, 184, 72, 176, 32, 134, 238, 230, 166, 97, 97, 165, 26, 113, 211, 38, 133, 237, 134, 158, 209, 208, 38, 16, 239, 30, 169, 173, 118, 1, 50, 221, 228, 147, 110, 189, 161, 181, 150, 57, 186, 182, 224, 200, 40, 3, 13, 99, 144, 166, 58, 69, 148, 117, 178, 97, 38, 31, 55, 145, 65, 153, 130, 252, 238, 225, 171, 56, 105, 37, 157, 151, 243, 152, 74, 28, 191, 216, 82, 14, 246, 191, 7, 249, 212, 153, 96, 160, 192, 144, 201, 151, 243, 22, 129, 16, 68, 79, 120, 73, 61, 189, 141, 66, 212, 172, 128, 192, 33, 157, 187, 150, 235, 158, 155, 102, 127, 95, 102, 223, 138, 212, 85, 60, 11, 48, 45, 108, 208, 68, 113, 95, 175, 211, 150, 16, 47, 137, 26, 0, 240, 154, 100, 141, 32, 128, 181, 133, 65, 106, 128, 87, 150, 127, 214, 221, 1, 121, 134, 23, 164, 120, 91, 143, 123, 158, 9, 22, 175, 248, 59, 22, 183, 109, 162, 213, 195, 70, 119, 48, 72, 108, 215, 79, 137, 151, 193, 54, 122, 59, 28, 134, 179, 124, 125, 154, 19, 170, 21, 38, 41, 113, 120, 164, 243, 40, 134, 90, 9, 238, 144, 70, 138, 254, 104, 202, 197, 147, 19, 13, 172, 205, 44, 145, 192, 101, 70, 120, 45, 110, 2, 227, 236, 119, 120, 186, 172, 197, 63, 20, 239, 70, 13, 1, 143, 149, 70, 9, 5, 190, 210, 158, 59, 76, 33, 212, 245, 162, 140, 174, 185, 87, 176, 236, 138, 255, 40, 99, 36, 35, 22, 249, 82, 169, 44, 71, 184, 206, 45, 250, 103, 249, 24, 207, 255, 117, 73, 95, 120, 139, 236, 218, 146, 155, 94, 196, 46, 227, 187, 97, 31, 139, 97, 228, 195, 67, 181, 112, 144, 170, 140, 247, 246, 108, 177, 227, 90, 183, 196, 192, 207, 203, 162, 45, 50, 239, 148, 103, 62, 183, 210, 220, 241, 55, 225, 167, 130, 172, 32, 150, 188, 84, 138, 61, 148, 158, 101, 68, 248, 84, 40, 104, 12, 82, 214, 20, 108, 25, 230, 45, 27, 71, 42, 78, 88, 112, 97, 252, 113, 64, 116, 122, 83, 35, 217, 10, 208, 141, 14, 46, 209, 46, 53, 211, 13, 162, 88, 90, 95, 190, 248, 189, 136, 54, 72, 199, 204, 105, 225, 146, 149, 234, 93, 31, 39, 219, 231, 27, 246, 72, 207, 105, 45, 82, 59, 226, 225, 186, 75, 199, 24, 231, 246, 60, 103, 78, 242, 163, 163, 77, 68, 68, 58, 1, 98, 61, 134, 24, 110, 155, 0, 204, 234, 46, 189, 7, 53, 201, 225, 73, 251, 145, 98, 253, 173, 181, 142, 128, 89, 3, 67, 1, 177, 48, 127, 198, 236, 250, 82, 246, 231, 250, 176, 40, 205, 71, 107, 200, 171, 147, 146, 243, 52, 207, 31, 117, 79, 79, 121, 155, 254, 144, 238, 144, 63, 95, 17, 200, 245, 155, 68, 113, 71, 254, 46, 221, 198, 112, 68, 194, 49, 67, 199, 187, 39, 99, 121, 11, 117, 241, 242, 221, 35, 74, 13, 92, 123, 91, 63, 168, 253, 186, 109, 226, 99, 173, 194, 93, 29, 74, 228, 1, 248, 165, 76, 5, 72, 175, 123, 245, 60, 137, 122, 29, 80, 175, 89, 16, 233, 88, 41, 187, 228, 102, 59, 134, 5, 103, 11, 139, 87, 57, 31, 170, 148, 69, 39, 206, 2, 229, 230, 167, 51, 91, 91, 156, 166, 161, 82, 165, 88, 203, 180, 104, 138, 157, 145, 197, 80, 237, 250, 163, 186, 125, 152, 19, 175, 173, 49, 226, 104, 70, 114, 158, 193, 89, 93, 116, 72, 122, 181, 222, 112, 159, 41, 211, 63, 125, 160, 144, 116, 163, 141, 88, 72, 34, 3, 143, 17, 127, 210, 254, 148, 44, 206, 36, 188, 149, 245, 81, 233, 67, 200, 230, 146, 33, 73, 64, 222, 229, 149, 124, 118, 58, 101, 107, 1, 122, 194, 46, 137, 117, 80, 166, 151, 207, 207, 233, 76, 81, 188, 105, 8, 240, 106, 110, 227, 190, 145, 29, 5, 128, 111, 116, 91, 24, 133, 245, 110, 84, 209, 194, 57, 148, 7, 52, 88, 32, 79, 115, 88, 13, 164, 152, 211, 34, 33, 190, 79, 237, 158, 184, 63, 122, 60, 41, 85, 118, 50, 162, 211, 184, 25, 9, 105, 214, 183, 240, 68, 155, 191, 68, 197, 101, 53, 206, 142, 164, 37, 35, 201, 222, 91, 207, 91, 43, 157, 177, 161, 76, 227, 60, 151, 10, 137, 101, 198, 174, 165, 201, 213, 133, 225, 54, 152, 46, 209, 122, 224, 175, 120, 22, 105, 251, 118, 143, 144, 30, 202, 72, 167, 11, 45, 236, 63, 78, 123, 33, 147, 45, 101, 209, 222, 136, 70, 126, 128, 119, 114, 234, 99, 191, 211, 76, 245, 70, 160, 122, 74, 133, 32, 85, 202, 168, 115, 50, 192, 32, 32, 1, 223, 25, 131, 179, 254, 0, 255, 219, 63, 209, 188, 205, 134, 167, 151, 124, 82, 230, 157, 216, 62, 87, 244, 177, 249, 232, 254, 224, 30, 233, 176, 126, 244, 57, 44, 164, 40, 82, 120, 74, 52, 241, 103, 97, 93, 31, 88, 222, 11, 102, 113, 49, 38, 209, 42, 225, 184, 139, 81, 17, 18, 207, 70, 49, 75, 109, 137, 131, 63, 6, 214, 6, 81, 45, 162, 81, 164, 136, 152, 60, 85, 108, 105, 88, 143, 0, 100, 147, 27, 153, 136, 209, 52, 224, 250, 27, 60, 68, 213, 105, 218, 62, 27, 53, 52, 127, 152, 152, 102, 0, 98, 48, 112, 34, 110, 55, 86, 49, 225, 97, 249, 185, 171, 187, 121, 95, 253, 48, 251, 126, 213, 0, 194, 169, 73, 185, 172, 25, 142, 87, 77, 92, 205, 16, 25, 28, 252, 121, 247, 212, 46, 119, 206, 64, 79, 213, 179, 193, 225, 49, 60, 9, 98, 53, 104, 19, 42, 53, 113, 169, 156, 238, 152, 20, 123, 152, 254, 180, 38, 74, 163, 37, 141, 132, 183, 86, 67, 34, 116, 104, 203, 43, 174, 179, 6, 115, 216, 88, 47, 69, 108, 55, 229, 209, 174, 194, 234, 155, 164, 224, 215, 51, 96, 103, 75, 246, 207, 101, 198, 168, 149, 92, 246, 66, 174, 116, 215, 134, 192, 208, 156, 105, 237, 7, 5, 72, 72, 205, 71, 194, 250, 216, 137, 56, 23, 52, 92, 31, 181, 236, 225, 18, 47, 122, 70, 32, 204, 97, 44, 153, 212, 204, 219, 7, 114, 125, 188, 47, 59, 63, 129, 57, 117, 92, 255, 75, 206, 238, 81, 214, 79, 176, 96, 250, 246, 141, 50, 137, 192, 116, 90, 74, 49, 89, 143, 225, 147, 2, 239, 153, 22, 110, 146, 53, 114, 132, 242, 212, 205, 115, 158, 16, 112, 171, 103, 254, 81, 47, 42, 94, 254, 155, 123, 121, 213, 81, 240, 72, 183, 36, 182, 6, 110, 42, 27, 188, 187, 128, 85, 89, 115, 65, 229, 123, 26, 31, 68, 95, 56, 68, 247, 167, 70, 96, 138, 89, 223, 119, 119, 78, 251, 249, 73, 69, 198, 161, 251, 140, 80, 232, 72, 250, 176, 174, 146, 73, 49, 142, 150, 151, 190, 22, 26, 73, 240, 29, 126, 167, 203, 174, 195, 190, 232, 12, 19, 214, 116, 104, 56, 4, 75, 188, 136, 107, 216, 93, 242, 123, 188, 46, 226, 241, 43, 50, 88, 60, 140, 88, 224, 78, 92, 84, 133, 74, 43, 253, 69, 89, 232, 35, 102, 141, 21, 54, 77, 21, 118, 76, 54, 206, 140, 41, 206, 34, 168, 2, 227, 130, 140, 52, 156, 210, 226, 0, 129, 238, 210, 191, 245, 238, 226, 60, 190, 225, 206, 234, 100, 97, 125, 51, 85, 216, 3, 37, 101, 108, 98, 78, 242, 103, 173, 221, 11, 130, 5, 203, 142, 190, 85, 254, 184, 112, 78, 96, 107, 35, 156, 159, 66, 78, 86, 221, 98, 153, 11, 194, 154, 111, 203, 142, 136, 189, 69, 107, 26, 156, 190, 19, 114, 0, 41, 124, 10, 155, 240, 98, 197, 96, 245, 51, 180, 225, 180, 112, 0, 24, 252, 64, 207, 155, 33, 151, 156, 154, 232, 181, 47, 18, 140, 223, 204, 82, 81, 34, 210, 120, 227, 75, 163, 58, 149, 215, 157, 222, 143, 235, 13, 100, 81, 116, 226, 197, 217, 214, 110, 161, 121, 243, 33, 197, 177, 212, 104, 144, 194, 137, 149, 204, 49, 57, 173, 150, 210, 55, 69, 155, 19, 243, 16, 53, 200, 183, 5, 8, 114, 141, 115, 73, 37, 250, 233, 48, 172, 125, 12, 75, 182, 59, 214, 90, 158, 251, 157, 228, 238, 44, 59, 15, 152, 66, 59, 141, 24, 238, 73, 40, 20, 210, 85, 111, 204, 152, 83, 145, 2, 26, 22, 20, 108, 11, 254, 9, 250, 79, 90, 69, 109, 99, 81, 38, 242, 11, 47, 113, 246, 193, 248, 156, 27, 23, 178, 196, 175, 76, 231, 126, 139, 160, 53, 17, 174, 104, 160, 214, 161, 50, 226, 54, 160, 200, 40, 162, 97, 88, 213, 228, 15, 47, 167, 0, 124, 2, 236, 129, 166, 119, 9, 92, 237, 169, 221, 130, 115, 218, 223, 156, 160, 28, 72, 100, 190, 236, 227, 178, 139, 183, 43, 47, 74, 224, 185, 64, 124, 76, 232, 208, 84, 157, 243, 154, 51, 71, 121, 12, 35, 178, 156, 138, 140, 194, 99, 96, 69, 177, 6, 181, 173, 163, 212, 9, 190, 48, 196, 53, 17, 190, 104, 130, 35, 223, 75, 2, 65, 102, 237, 246, 234, 66, 14, 175, 87, 242, 227, 127, 83, 153, 98, 69, 219, 234, 101, 216, 120, 235, 138, 171, 102, 26, 217, 145, 104, 144, 168, 254, 62, 137, 5, 209, 3, 134, 219, 179, 202, 47, 106, 142, 207, 38, 68, 222, 77, 177, 231, 237, 127, 143, 136, 177, 159, 110, 202, 21, 56, 3, 241, 188, 232, 95, 24, 74, 137, 130, 136, 242, 32, 89, 19, 26, 249, 75, 94, 237, 9, 213, 130, 25, 205, 71, 229, 240, 148, 99, 105, 158, 238, 234, 225, 124, 73, 14, 21, 177, 154, 183, 218, 241, 176, 197, 94, 185, 190, 85, 65, 63, 197, 170, 75, 113, 59, 168, 192, 38, 100, 59, 232, 140, 232, 205, 219, 6, 137, 36, 230, 211, 223, 223, 103, 0, 233, 57, 151, 62, 65, 114, 152, 104, 103, 117, 40, 174, 55, 10, 64, 20, 166, 80, 3, 7, 113, 46, 29, 117, 49, 129, 57, 204, 123, 57, 15, 245, 223, 246, 134, 221, 185, 200, 22, 174, 253, 154, 189, 7, 16, 234, 195, 230, 14, 50, 114, 132, 118, 163, 23, 156, 208, 151, 71, 111, 214, 187, 111, 92, 83, 177, 192, 97, 154, 106, 202, 74, 44, 178, 175, 248, 180, 36, 217, 55, 26, 200, 23, 42, 104, 83, 64, 43, 58, 67, 74, 232, 31, 203, 44, 247, 253, 148, 213, 81, 167, 116, 99, 70, 108, 21, 83, 224, 239, 196, 150, 87, 216, 91, 255, 243, 254, 180, 229, 234, 150, 129, 190, 90, 248, 59, 64, 182, 36, 23, 194, 106, 242, 69, 209, 85, 115, 165, 182, 182, 231, 72, 81, 9, 56, 215, 208, 255, 213, 127, 251, 33, 145, 89, 230, 235, 55, 218, 214, 23, 206, 163, 5, 3, 65, 215, 37, 100, 180, 57, 170, 197, 216, 164, 35, 224, 90, 92, 143, 181, 203, 177, 172, 166, 155, 209, 181, 140, 199, 121, 34, 242, 175, 71, 23, 89, 171, 27, 191, 18, 221, 229, 172, 3, 165, 49, 30, 135, 152, 136, 122, 38, 90, 123, 209, 177, 129, 3, 239, 248, 127, 115, 159, 132, 69, 145, 10, 225, 235, 145, 186, 234, 22, 12, 250, 189, 10, 124, 119, 196, 41, 57, 213, 67, 34, 45, 55, 135, 247, 75, 147, 79, 0, 214, 7, 128, 167, 149, 139, 4, 32, 62, 100, 238, 163, 106, 141, 130, 206, 34, 158, 16, 176, 231, 146, 128, 196, 146, 216, 135, 106, 28, 7, 151, 106, 231, 226, 229, 182, 81, 172, 220, 233, 214, 243, 215, 20, 71, 182, 95, 99, 34, 150, 13, 247, 91, 200, 179, 118, 138, 18, 66, 100, 177, 214, 94, 174, 120, 236, 254, 163, 8, 73, 3, 3, 49, 131, 26, 169, 0, 29, 132, 154, 93, 195, 103, 254, 146, 206, 31, 36, 130, 178, 223, 187, 240, 162, 236, 76, 64, 65, 88, 49, 134, 33, 63, 189, 170, 117, 28, 180, 95, 120, 71, 148, 86, 218, 254, 230, 214, 39, 99, 45, 224, 235, 116, 221, 228, 149, 72, 222, 166, 36, 48, 73, 29, 78, 228, 192, 25, 113, 79, 238, 135, 190, 75, 164, 120, 63, 182, 80, 6, 12, 31, 56, 239, 189, 211, 212, 19, 177, 157, 32, 212, 78, 118, 158, 247, 111, 209, 110, 126, 232, 205, 160, 5, 144, 171, 59, 58, 254, 80, 241, 4, 163, 160, 89, 103, 73, 64, 239, 75, 2, 198, 9, 26, 146, 144, 80, 63, 223, 163, 204, 48, 205, 4, 241, 83, 45, 28, 11, 214, 133, 121, 48, 126, 176, 233, 91, 207, 99, 202, 29, 232, 163, 85, 61, 204, 184, 102, 132, 17, 63, 253, 88, 112, 65, 227, 127, 172, 122, 176, 55, 152, 79, 154, 98, 186, 242, 74, 85, 21, 227, 209, 241, 158, 235, 45, 44, 34, 125, 97, 115, 89, 196, 150, 148, 85, 103, 22, 184, 12, 15, 64, 9, 164, 29, 13, 40, 81, 81, 53, 211, 194, 37, 63, 233, 229, 149, 235, 101, 202, 140, 101, 123, 182, 174, 172, 238, 105, 171, 64, 24, 40, 206, 76, 214, 9, 56, 41, 99, 242, 91, 88, 56, 69, 83, 152, 117, 215, 147, 78, 254, 124, 251, 150, 255, 48, 157, 111, 153, 66, 147, 17, 157, 192, 129, 224, 198, 229, 231, 134, 246, 15, 140, 185, 60, 15, 72, 20, 112, 6, 39, 168, 65, 5, 124, 12, 23, 235, 237, 75, 64, 58, 195, 168, 183, 158, 176, 158, 114, 176, 90, 187, 115, 158, 245, 97, 154, 191, 189, 250, 230, 46, 61, 57, 176, 1, 187, 126, 45, 50, 244, 179, 77, 44, 220, 102, 179, 147, 142, 61, 178, 249, 66, 93, 124, 167, 140, 237, 120, 178, 223, 32, 173, 108, 181, 7, 79, 14, 126, 187, 157, 208, 214, 30, 120, 207, 158, 115, 157, 228, 185, 243, 188, 228, 52, 71, 9, 46, 181, 17, 196, 173, 149, 23, 204, 126, 201, 24, 18, 235, 32, 40, 253, 190, 132, 246, 136, 182, 105, 1, 46, 162, 72, 95, 49, 74, 32, 126, 80, 251, 236, 2, 41, 202, 159, 19, 81, 154, 172, 95, 200, 159, 74, 171, 215, 177, 143, 67, 54, 4, 129, 155, 84, 100, 55, 143, 171, 113, 91, 148, 180, 162, 39, 114, 179, 214, 141, 73, 205, 68, 162, 15, 64, 191, 77, 219, 20, 46, 148, 146, 4, 157, 16, 146, 147, 207, 53, 102, 209, 2, 215, 13, 51, 188, 131, 104, 139, 72, 111, 220, 147, 84, 5, 137, 141, 8, 145, 221, 126, 89, 79, 118, 22, 204, 235, 75, 200, 46, 184, 37, 157, 37, 148, 203, 109, 226, 57, 167, 61, 118, 204, 52, 69, 114, 11, 142, 158, 72, 28, 248, 212, 225, 242, 229, 197, 91, 47, 218, 89, 1, 89, 161, 175, 138, 240, 49, 191, 152, 35, 2, 80, 215, 145, 202, 60, 126, 26, 24, 190, 244, 231, 94, 222, 48, 95, 134, 149, 232, 17, 36, 204, 226, 181, 31, 224, 251, 161, 82, 91, 212, 198, 224, 39, 144, 75, 149, 183, 5, 90, 68, 135, 137, 26, 143, 122, 92, 251, 41, 145, 240, 113, 154, 191, 2, 166, 223, 89, 148, 221, 225, 229, 149, 155, 42, 124, 129, 189, 30, 27, 174, 84, 0, 95, 153, 214, 148, 71, 137, 239, 179, 129, 164, 78, 206, 245, 132, 52, 186, 247, 204, 32, 164, 181, 88, 60, 51, 28, 163, 76, 192, 23, 39, 240, 143, 50, 239, 174, 101, 27, 205, 38, 185, 249, 176, 60, 131, 104, 51, 90, 232, 171, 84, 196, 7, 253, 172, 5, 176, 241, 22, 102, 145, 35, 226, 252, 28, 55, 241, 10, 36, 76, 100, 202, 106, 183, 63, 166, 149, 250, 19, 110, 182, 66, 86, 188, 106, 99, 199, 171, 209, 245, 147, 150, 99, 240, 147, 97, 155, 28, 137, 155, 239, 201, 217, 246, 163, 246, 226, 213, 121, 33, 234, 50, 46, 83, 35, 55, 179, 108, 187, 201, 117, 206, 70, 51, 181, 246, 127, 91, 120, 126, 38, 79, 174, 29, 236, 80, 23, 137, 115, 158, 217, 157, 137, 249, 88, 106, 55, 70, 115, 3, 90, 14, 197, 223, 79, 140, 19, 148, 21, 150, 37, 50, 87, 210, 245, 166, 137, 147, 86, 36, 171, 107, 219, 213, 11, 77, 195, 45, 212, 31, 113, 144, 247, 134, 100, 219, 35, 177, 97, 163, 20, 70, 170, 23, 5, 159, 200, 118, 249, 216, 206, 123, 205, 9, 15, 172, 101, 199, 142, 36, 56, 231, 136, 113, 185, 246, 215, 115, 213, 97, 208, 209, 85, 114, 102, 253, 249, 168, 8, 206, 141, 63, 205, 116, 17, 206, 32, 115, 121, 142, 106, 78, 18, 86, 74, 98, 46, 252, 134, 113, 239, 112, 147, 52, 236, 54, 226, 235, 238, 85, 203, 19, 126, 166, 221, 223, 200, 109, 59, 48, 40, 57, 42, 148, 228, 80, 6, 159, 212, 15, 207, 132, 128, 43, 40, 212, 135, 139, 24, 150, 212, 156, 184, 44, 184, 188, 92, 198, 95, 134, 55, 246, 162, 33, 35, 207, 39, 232, 199, 70, 63, 123, 223, 47, 125, 115, 53, 99, 234, 36, 238, 93, 4, 231, 114, 244, 168, 42, 34, 51, 166, 68, 86, 222, 158, 100, 107, 191, 33, 179, 146, 140, 162, 112, 106, 129, 134, 196, 139, 42, 85, 149, 31, 77, 88, 130, 84, 85, 64, 24, 219, 30, 203, 40, 127, 155, 255, 252, 140, 198, 51, 15, 247, 115, 200, 187, 170, 97, 48, 196, 122, 158, 32, 22, 211, 172, 143, 25, 174, 236, 171, 181, 58, 113, 139, 40, 83, 205, 101, 188, 78, 67, 234, 191, 14, 103, 94, 115, 127, 149, 98, 12, 55, 49, 83, 42, 167, 111, 47, 168, 38, 174, 207, 73, 42, 33, 181, 14, 254, 86, 92, 178, 199, 42, 54, 12, 215, 248, 11, 92, 69, 94, 15, 24, 227, 177, 207, 15, 108, 10, 33, 19, 6, 169, 128, 134, 203, 244, 105, 132, 145, 217, 203, 113, 81, 202, 30, 26, 132, 168, 24, 203, 26, 108, 84, 28, 59, 166, 233, 147, 48, 161, 230, 211, 147, 240, 134, 166, 240, 105, 229, 53, 80, 245, 195, 98, 9, 207, 183, 80, 222, 169, 149, 94, 84, 30, 117, 86, 122, 23, 122, 194, 250, 46, 241, 14, 197, 148, 219, 131, 172, 7, 190, 164, 180, 30, 211, 229, 14, 52, 71, 249, 211, 168, 229, 219, 6, 77, 65, 14, 154, 156, 144, 240, 40, 253, 132, 217, 160, 82, 127, 58, 128, 44, 193, 102, 231, 165, 83, 180, 233, 74, 67, 12, 196, 132, 16, 42, 10, 99, 255, 23, 4, 25, 156, 88, 72, 55, 142, 237, 220, 81, 78, 49, 228, 238, 138, 121, 107, 253, 182, 221, 120, 171, 47, 234, 48, 128, 2, 21, 32, 204, 229, 105, 90, 44, 24, 116, 10, 101, 78, 225, 100, 158, 171, 241, 15, 170, 201, 9, 100, 6, 179, 101, 81, 167, 113, 232, 111, 181, 165, 166, 118, 250, 6, 43, 122, 211, 226, 197, 181, 3, 186, 67, 175, 28, 141, 222, 17, 197, 155, 222, 226, 220, 139, 31, 240, 178, 148, 87, 92, 23, 253, 123, 147, 95, 234, 22, 123, 57, 208, 111, 200, 79, 59, 11, 61, 245, 126, 91, 252, 66, 125, 13, 76, 204, 69, 3, 75, 158, 140, 170, 250, 42, 201, 175, 222, 248, 47, 250, 196, 49, 1, 6, 50, 17, 186, 68, 33, 185, 176, 118, 251, 201, 58, 194, 220, 123, 125, 56, 106, 92, 121, 151, 64, 71, 237, 52, 65, 112, 69, 173, 34, 160, 46, 13, 148, 147, 23, 20, 44, 130, 154, 247, 129, 177, 156, 70, 199, 97, 217, 152, 144, 34, 124, 139, 71, 213, 223, 196, 226, 250, 7, 200, 181, 12, 134, 41, 169, 89, 14, 211, 203, 119, 55, 82, 66, 149, 242, 200, 5, 156, 161, 250, 118, 124, 87, 4, 109, 172, 17, 40, 79, 244, 72, 47, 138, 217, 55, 208, 200, 105, 49, 122, 25, 102, 65, 114, 179, 11, 136, 79, 72, 135, 230, 245, 193, 48, 226, 152, 217, 225, 173, 124, 24, 56, 135, 72, 65, 78, 184, 248, 111, 162, 241, 24, 163, 30, 76, 166, 45, 229, 54, 168, 73, 53, 56, 21, 167, 89, 195, 198, 96, 188, 200, 195, 147, 137, 219, 113, 236, 65, 26, 71, 14, 212, 13, 80, 159, 220, 225, 46, 10, 122, 212, 141, 34, 20, 138, 67, 123, 18, 208, 4, 164, 23, 226, 45, 60, 239, 87, 65, 1, 181, 223, 39, 108, 62, 47, 156, 171, 29, 16, 215, 127, 66, 126, 131, 137, 109, 169, 46, 9, 194, 192, 39, 143, 8, 17, 113, 154, 93, 134, 2, 149, 120, 229, 20, 110, 128, 224, 250, 60, 3, 110, 142, 173, 127, 8, 239, 223, 245, 108, 109, 144, 205, 160, 233, 206, 162, 154, 10, 224, 105, 67, 186, 78, 54, 136, 253, 177, 245, 126, 2, 232, 171, 132, 221, 32, 153, 59, 154, 92, 44, 80, 230, 28, 43, 116, 153, 68, 25, 8, 13, 83, 86, 22, 4, 109, 238, 144, 145, 16, 13, 199, 158, 197, 87, 251, 7, 134, 143, 109, 211, 73, 10, 172, 228, 14, 30, 75, 107, 94, 208, 74, 20, 123, 224, 53, 166, 66, 230, 54, 234, 158, 79, 120, 163, 35, 218, 108, 77, 219, 105, 167, 117, 120, 244, 207, 98, 195, 55, 187, 171, 54, 171, 23, 240, 93, 212, 204, 141, 183, 85, 32, 112, 171, 15, 195, 128, 52, 224, 253, 106, 203, 130, 123, 245, 46, 100, 149, 215, 124, 71, 244, 195, 196, 44, 252, 234, 113, 57, 52, 210, 6, 22, 115, 171, 1, 199, 131, 56, 215, 214, 220, 232, 128, 138, 221, 214, 208, 14, 11, 213, 79, 218, 36, 71, 49, 95, 41, 13, 253, 98, 196, 151, 158, 198, 31, 101, 33, 28, 231, 162, 198, 95, 225, 76, 92, 239, 21, 96, 223, 98, 16, 87, 207, 244, 55, 210, 16, 52, 188, 217, 4, 92, 149, 74, 174, 227, 61, 134, 233, 66, 250, 32, 165, 232, 163, 70, 124, 71, 16, 216, 125, 253, 19, 194, 170, 32, 147, 165, 60, 160, 249, 223, 117, 106, 133, 209, 18, 174, 92, 83, 190, 61, 165, 190, 97, 164, 239, 29, 117, 51, 52, 141, 51, 64, 158, 90, 165, 59, 250, 100, 121, 221, 165, 57, 20, 76, 192, 131, 41, 230, 239, 166, 105, 91, 10, 207, 2, 73, 102, 59, 19, 231, 156, 33, 144, 142, 158, 10, 63, 82, 213, 111, 224, 210, 95, 120, 70, 203, 26, 24, 187, 131, 222, 8, 172, 248, 138, 191, 134, 19, 177, 166, 137, 160, 176, 233, 65, 128, 69, 225, 88, 210, 92, 1, 174, 137, 38, 67, 115, 235, 62, 173, 189, 122, 165, 101, 148, 242, 44, 157, 150, 67, 39, 209, 204, 166, 108, 146, 0, 140, 179, 20, 190, 250, 252, 241, 243, 221, 45, 21, 183, 155, 100, 239, 140, 81, 179, 156, 16, 9, 163, 191, 67, 128, 93, 34, 183, 217, 2, 84, 252, 228, 44, 239, 54, 193, 179, 243, 23, 153, 255, 115, 160, 66, 3, 215, 60, 178, 223, 17, 30, 159, 1, 101, 114, 135, 209, 39, 156, 253, 159, 239, 131, 125, 110, 61, 231, 43, 150, 142, 187, 207, 30, 126, 53, 68, 0, 128, 144, 35, 116, 254, 16, 39, 231, 96, 250, 152, 81, 181, 51, 59, 252, 16, 49, 252, 238, 76, 26, 173, 10, 5, 149, 118, 5, 215, 131, 83, 141, 51, 119, 175, 59, 211, 221, 76, 89, 207, 205, 112, 189, 10, 22, 24, 110, 91, 77, 2, 132, 33, 241, 210, 232, 172, 63, 111, 195, 113, 114, 71, 143, 188, 157, 245, 37, 86, 45, 116, 52, 213, 195, 31, 181, 22, 94, 27, 224, 186, 43, 138, 39, 254, 121, 64, 86, 63, 114, 255, 1, 6, 137, 203, 95, 158, 5, 227, 68, 235, 105, 176, 80, 242, 90, 195, 74, 128, 0, 208, 140, 121, 205, 76, 157, 87, 146, 130, 64, 174, 211, 115, 190, 133, 110, 243, 39, 45, 161, 4, 53, 224, 134, 49, 219, 199, 216, 155, 214, 18, 40, 38, 140, 134, 46, 40, 90, 45, 130, 239, 153, 54, 113, 201, 125, 38, 237, 219, 206, 234, 247, 199, 144, 26, 167, 41, 184, 252, 143, 52, 133, 144, 67, 131, 232, 154, 215, 198, 176, 101, 134, 93, 167, 95, 81, 78, 224, 0, 90, 123, 107, 139, 77, 82, 134, 83, 217, 164, 106, 206, 128, 209, 210, 134, 94, 114, 94, 165, 72, 150, 111, 182, 33, 77, 191, 218, 189, 73, 193, 3, 63, 85, 187, 126, 41, 30, 113, 40, 23, 11, 100, 0, 75, 193, 108, 141, 149, 114, 2, 206, 127, 248, 62, 192, 93, 39, 47, 196, 176, 75, 161, 66, 145, 138, 162, 119, 141, 180, 128, 35, 47, 238, 159, 213, 94, 115, 159, 22, 235, 119, 36, 118, 248, 21, 94, 225, 38, 253, 188, 151, 109, 191, 233, 158, 162, 93, 174, 159, 11, 216, 197, 168, 11, 219, 27, 184, 124, 239, 127, 9, 47, 50, 137, 51, 245, 231, 45, 187, 175, 213, 251, 108, 21, 176, 100, 132, 221, 1, 54, 228, 22, 45, 168, 27, 78, 118, 7, 58, 121, 162, 203, 30, 201, 4, 246, 154, 104, 124, 94, 181, 67, 6, 87, 96, 141, 39, 187, 134, 218, 106, 176, 130, 136, 81, 179, 247, 102, 169, 245, 159, 30, 84, 107, 44, 238, 99, 85, 53, 140, 192, 238, 79, 227, 193, 136, 215, 154, 232, 141, 213, 88, 113, 208, 156, 86, 123, 210, 229, 151, 247, 75, 188, 12, 239, 81, 179, 43, 234, 210, 145, 144, 236, 253, 21, 112, 13, 22, 138, 171, 6, 62, 91, 231, 136, 64, 161, 74, 245, 245, 195, 7, 202, 209, 68, 31, 165, 140, 219, 194, 160, 10, 27, 14, 29, 39, 62, 4, 84, 133, 252, 135, 218, 136, 3, 57, 115, 4, 235, 134, 130, 62, 178, 40, 49, 14, 63, 220, 56, 119, 33, 102, 105, 230, 176, 215, 171, 178, 140, 219, 228, 204, 60, 144, 215, 173, 22, 57, 55, 182, 136, 210, 204, 154, 63, 232, 111, 185, 67, 234, 144, 221, 121, 167, 214, 38, 106, 102, 2, 21, 242, 206, 142, 0, 92, 130]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "Lo = 8;   #Total system losses in dB\n",
        "Nf = 5;    #Mobile receiver noise figure in dB\n",
        "T = 290;   #temperature in degree kelvin\n",
        "BW = 10*10**6; #in Hz\n",
        "Gb = 8;  #in dB\n",
        "Gm = 0;   #in dB\n",
        "Hb = 1;  #in metres\n",
        "Hm = 1.1;   #in metres\n",
        "B = 1.38*10**-23; #Boltzmann's constant\n",
        "Te = T*(3.162-1)\n",
        "Pn = B*(Te+T)*BW\n",
        "Free_Lp = {}\n",
        "Pr = {}\n",
        "SNR_var = {}\n",
        "ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "s = np.floor(np.random.uniform(0,256, 1000))\n",
        "s = s.astype('int64')            \n",
        "for i in range(1000):\n",
        "  Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "  Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_std.append(i)\n",
        "\n",
        "ad_noise_std = np.transpose(np.tile(ad_noise_std, (2,2,1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0hIseee8ira",
        "outputId": "43e9ea14-a808-45a1-85c0-f25c5a210909"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s) \n",
        "\n",
        "                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "89b4da87-b0a1-4857-9796-8e2356f93775"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , 0.01, train_EbNodB, 10000]\n",
        "#    [10000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[52.82986394 52.82986394]\n",
            "  [52.82986394 52.82986394]]\n",
            "\n",
            " [[39.38317184 39.38317184]\n",
            "  [39.38317184 39.38317184]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[43.64638631 43.64638631]\n",
            "  [43.64638631 43.64638631]]\n",
            "\n",
            " [[51.69651999 51.69651999]\n",
            "  [51.69651999 51.69651999]]\n",
            "\n",
            " [[51.69651999 51.69651999]\n",
            "  [51.69651999 51.69651999]]], Iterations: 10000\n",
            "0.956\n",
            "0.06800002\n",
            "0.04400003\n",
            "0.04699999\n",
            "0.04100001\n",
            "0.037\n",
            "0.032999992\n",
            "0.038999975\n",
            "0.01999998\n",
            "0.022000015\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13e33fb-bbe9-47d6-af4b-958ca193b4b0"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s) \n",
        "\n",
        "                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(2))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjh_J3MIlyrF",
        "outputId": "9e31a558-f1cd-4077-fca2-0cb17e6d061c"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , 0.01, train_EbNodB, 10000]\n",
        "#    [50000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[52.82986394 52.82986394]\n",
            "  [52.82986394 52.82986394]]\n",
            "\n",
            " [[39.38317184 39.38317184]\n",
            "  [39.38317184 39.38317184]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[43.64638631 43.64638631]\n",
            "  [43.64638631 43.64638631]]\n",
            "\n",
            " [[51.69651999 51.69651999]\n",
            "  [51.69651999 51.69651999]]\n",
            "\n",
            " [[51.69651999 51.69651999]\n",
            "  [51.69651999 51.69651999]]], Iterations: 10000\n",
            "0.975\n",
            "0.21100003\n",
            "0.199\n",
            "0.181\n",
            "0.18599999\n",
            "0.17500001\n",
            "0.13700002\n",
            "0.13300002\n",
            "0.13499999\n",
            "0.12900001\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "5c35c270-1e65-42cf-ef32-b1567f812bb3"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        #weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
        "        weights = np.arange(M)\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "plot_constellation_2(ae,range(0,ae.M))\n",
        "#plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-82baf4338289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-82baf4338289>\u001b[0m in \u001b[0;36mplot_constellation_2\u001b[0;34m(ae, arr, maxrange)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m#marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#weights = [1,4,9,16,25,36,49,64]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4da0ba2493b1>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), 40, tr)\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), 40, tr)\n",
        "#print(np.shape(tr))\n",
        "#print(np.shape(tr_hat))\n",
        "print(tr)\n",
        "print(tr_hat_w)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rmse_uw = {}\n",
        "for j in range(M):\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "rmse_w = {}\n",
        "for j in range(M):\n",
        "  rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "\n",
        "\n",
        "#message_factor = [15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1]\n",
        "message_factor = np.flip(np.arange(M))\n",
        "message_factor[M-1] = 1\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "    \n",
        "#rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7, rmse_uw_8, rmse_uw_9, rmse_uw_10, rmse_uw_11, rmse_uw_12, rmse_uw_13, rmse_uw_14, rmse_uw_15]\n",
        "rmse_uw = (np.divide(rmse_uwA,message_factor))\n",
        "#rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7, rmse_w_8, rmse_w_9, rmse_w_10, rmse_w_11, rmse_w_12, rmse_w_13, rmse_w_14, rmse_w_15]\n",
        "rmse_w = (np.divide(rmse_wA,message_factor))\n",
        "#message = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "message = np.arange(M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE deviation log2')\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])\n",
        "\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "\n",
        "print(np.sum(rmse_uwA))\n",
        "print(np.sum(rmse_wA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i"
      },
      "source": [
        "ae.plot_constellation();\n",
        "#plot_constellation_2(ae,range(0,ae.M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75eUVLFnq1JO"
      },
      "source": [
        "ae_Weighted.plot_constellation();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukzCBJff7iE"
      },
      "source": [
        " ebnodbs = np.linspace(0,14,15)\n",
        "BLER_8PSK = [0.3478959, 0.2926128, 0.2378847, 0.1854187, 0.1372344, 0.0953536, 0.0614003, 0.0360195, 0.0185215, 0.0082433, 0.0030178, 0.0008626, 0.0001903, 0.0000289, 0.0000027, ]\n",
        "blers = ae.bler_sim(ebnodbs, 1000000, 1);\n",
        "ae.plot_bler(ebnodbs, blers);\n",
        "blers_w = ae_Weighted.bler_sim(ebnodbs, 1000000, 1);\n",
        "#ae_Weighted.plot_bler(ebnodbs, blers_w);\n",
        "plt.plot(ebnodbs, blers_w)\n",
        "plt.semilogy(snr_db,ser,'o')\n",
        "plt.plot(ebnodbs,BLER_8PSK);\n",
        "plt.legend(['Autoencoder (Rayleigh+AWGN)', 'Weighted Autoencoder(Rayleigh+AWGN)', 'SER Sim(AWGN)', '8PSK(AWGN)'], prop={'size': 16}, loc='lower left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        "np.divide(10,2)\n",
        "#IEEE 802.11p Max TX Power 30dBm\n",
        "#Path loss model corresponding: 20log10(d(m))  (We take n=2, FSPL)\n",
        "#0.1W corresponds to 20dBm or -10dB\n",
        "#7dB corresponds to 5W\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.850–5.925 GHz).\n",
        "# Noise Power -100dBm, TX Power is 20dBm and that \\\n",
        "#256m- -94dBm, 128m- -88dBm, 1m"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}