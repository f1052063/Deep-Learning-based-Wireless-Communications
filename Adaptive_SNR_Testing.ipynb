{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Adaptive_SNR_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Adaptive_SNR_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "4bf714ed-3a72-4990-b78a-d91c7932a680"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "c7af20aa-90a3-4f4a-83ea-abced5ae9903"
      },
      "source": [
        "batch_size = 10000\n",
        "########## BIT FLIPPING ON\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#tr2 = ([replacer(n, n) for n in triangle3])\n",
        "print(tr)\n",
        "#plt.plot(t, tr)\n",
        "#plt.legend(['Input signal', 'Quantized and bit flipped'])\n",
        "\n",
        "#s_ind = np.empty(shape=(M,batch_size))\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "\n",
        "#print([tr[x] for x in s_ind_0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[123. 160. 175. ... 231.  75. 129.]\n",
            "[132, 95, 80, 129, 174, 132, 47, 161, 22, 238, 89, 105, 159, 189, 245, 175, 115, 203, 19, 225, 184, 91, 63, 38, 158, 66, 138, 230, 229, 147, 218, 113, 123, 38, 175, 225, 173, 30, 142, 95, 240, 103, 201, 199, 75, 39, 185, 25, 51, 146, 200, 216, 133, 244, 58, 246, 58, 29, 227, 132, 150, 186, 96, 77, 21, 228, 104, 49, 231, 175, 101, 71, 32, 181, 32, 244, 235, 247, 96, 230, 101, 113, 7, 216, 238, 38, 47, 193, 190, 83, 254, 235, 104, 205, 14, 134, 155, 53, 191, 173, 138, 78, 135, 69, 165, 254, 154, 214, 2, 141, 104, 77, 60, 136, 89, 150, 118, 120, 44, 185, 152, 106, 171, 243, 112, 68, 69, 56, 212, 206, 171, 212, 152, 144, 183, 103, 225, 91, 205, 9, 85, 130, 131, 136, 198, 204, 90, 239, 116, 187, 161, 224, 91, 253, 245, 201, 36, 66, 182, 238, 255, 160, 158, 49, 96, 154, 11, 25, 45, 0, 27, 180, 135, 151, 181, 152, 8, 112, 219, 100, 197, 68, 19, 46, 64, 194, 179, 118, 189, 16, 177, 127, 98, 56, 8, 222, 42, 106, 125, 67, 120, 140, 90, 194, 26, 195, 222, 241, 128, 75, 134, 6, 145, 14, 90, 57, 182, 231, 81, 124, 99, 46, 68, 181, 55, 107, 230, 115, 238, 139, 201, 37, 94, 207, 205, 110, 170, 121, 143, 98, 222, 190, 79, 87, 129, 76, 67, 226, 6, 220, 65, 69, 192, 217, 200, 207, 44, 23, 32, 2, 45, 253, 142, 140, 110, 22, 5, 164, 225, 114, 113, 23, 145, 196, 69, 226, 153, 32, 46, 244, 74, 26, 166, 209, 81, 62, 78, 155, 109, 125, 47, 133, 222, 135, 138, 161, 84, 99, 177, 146, 109, 195, 247, 100, 8, 251, 158, 227, 22, 54, 211, 175, 210, 15, 133, 191, 39, 60, 229, 241, 55, 94, 45, 18, 96, 184, 188, 32, 158, 137, 73, 193, 136, 195, 199, 193, 131, 72, 142, 128, 35, 79, 147, 226, 221, 1, 238, 4, 93, 228, 47, 110, 249, 140, 130, 224, 243, 168, 204, 98, 59, 244, 98, 39, 183, 55, 191, 160, 171, 249, 39, 248, 240, 78, 196, 217, 44, 18, 121, 140, 201, 77, 125, 223, 212, 48, 23, 172, 91, 85, 163, 107, 254, 93, 202, 125, 135, 12, 161, 247, 31, 93, 210, 216, 119, 38, 221, 216, 104, 90, 191, 166, 81, 32, 3, 77, 72, 162, 151, 96, 87, 162, 62, 45, 115, 133, 58, 42, 135, 57, 190, 164, 235, 195, 3, 223, 62, 129, 196, 147, 76, 111, 83, 240, 27, 213, 166, 77, 71, 167, 201, 199, 142, 216, 170, 53, 139, 229, 196, 109, 105, 125, 248, 72, 207, 84, 250, 99, 40, 132, 50, 19, 56, 129, 254, 144, 166, 123, 215, 49, 129, 79, 96, 124, 176, 245, 176, 58, 171, 133, 211, 189, 118, 9, 120, 64, 171, 114, 251, 231, 29, 112, 86, 150, 11, 57, 77, 234, 206, 154, 74, 35, 49, 53, 234, 100, 135, 25, 47, 73, 3, 88, 105, 122, 98, 102, 193, 188, 122, 222, 7, 5, 155, 20, 176, 115, 14, 225, 30, 43, 44, 59, 138, 2, 99, 121, 208, 155, 39, 156, 18, 178, 123, 26, 107, 224, 25, 116, 128, 252, 242, 233, 82, 95, 220, 151, 147, 97, 185, 26, 165, 64, 118, 253, 0, 155, 37, 22, 140, 27, 24, 180, 160, 229, 69, 88, 193, 209, 227, 253, 148, 23, 23, 136, 156, 31, 181, 7, 52, 240, 143, 5, 166, 57, 230, 208, 134, 240, 33, 18, 53, 196, 72, 59, 213, 194, 198, 254, 248, 114, 157, 239, 168, 240, 43, 17, 245, 218, 51, 216, 177, 235, 31, 32, 24, 71, 71, 240, 212, 73, 78, 253, 207, 134, 14, 202, 176, 85, 136, 215, 153, 181, 70, 220, 14, 237, 52, 103, 188, 181, 134, 216, 119, 218, 251, 12, 169, 211, 113, 248, 201, 139, 174, 54, 133, 34, 132, 85, 229, 215, 188, 226, 74, 190, 157, 36, 139, 149, 73, 124, 64, 20, 190, 209, 187, 216, 139, 215, 24, 9, 5, 150, 49, 6, 50, 74, 58, 235, 146, 209, 26, 31, 144, 226, 189, 17, 37, 125, 67, 64, 90, 121, 87, 216, 7, 156, 75, 202, 239, 133, 244, 194, 90, 168, 92, 46, 69, 13, 162, 93, 31, 162, 19, 224, 255, 170, 154, 225, 18, 154, 77, 198, 138, 197, 68, 206, 236, 189, 3, 60, 252, 49, 140, 85, 206, 225, 150, 14, 22, 139, 46, 110, 18, 120, 155, 187, 185, 237, 245, 119, 44, 142, 194, 138, 87, 96, 147, 15, 152, 213, 55, 216, 134, 220, 167, 138, 34, 165, 25, 198, 155, 82, 84, 137, 135, 104, 129, 32, 165, 72, 48, 124, 225, 177, 36, 125, 68, 58, 7, 94, 67, 31, 114, 216, 146, 158, 224, 159, 200, 97, 179, 101, 100, 179, 237, 140, 230, 81, 200, 176, 201, 119, 114, 11, 67, 4, 59, 119, 76, 154, 117, 44, 104, 101, 158, 82, 140, 136, 135, 12, 111, 13, 116, 95, 142, 183, 115, 189, 78, 78, 236, 183, 231, 247, 254, 42, 7, 247, 226, 34, 3, 253, 223, 227, 60, 197, 117, 218, 234, 238, 242, 212, 40, 200, 232, 252, 197, 97, 189, 142, 207, 246, 183, 142, 155, 49, 163, 221, 75, 127, 204, 153, 239, 38, 25, 253, 238, 104, 169, 4, 79, 143, 15, 60, 112, 96, 183, 55, 33, 110, 48, 112, 34, 141, 139, 195, 61, 194, 103, 244, 203, 72, 33, 58, 11, 19, 209, 57, 161, 143, 243, 72, 217, 228, 81, 190, 6, 23, 218, 180, 176, 217, 207, 229, 50, 166, 8, 127, 186, 93, 95, 200, 235, 95, 111, 0, 228, 130, 215, 59, 23, 41, 139, 30, 166, 164, 60, 54, 109, 31, 112, 33, 145, 184, 57, 57, 66, 51, 159, 97, 32, 243, 26, 116, 42, 224, 179, 108, 41, 171, 199, 73, 173, 99, 93, 20, 73, 233, 66, 243, 90, 222, 223, 180, 115, 42, 230, 2, 134, 113, 24, 120, 54, 156, 140, 9, 129, 135, 148, 115, 45, 242, 135, 242, 180, 5, 107, 250, 151, 200, 102, 48, 47, 199, 22, 12, 19, 204, 24, 222, 189, 49, 202, 248, 149, 129, 32, 179, 192, 238, 204, 118, 60, 8, 65, 201, 8, 49, 199, 122, 158, 197, 210, 111, 200, 219, 18, 80, 156, 185, 78, 118, 4, 36, 68, 203, 150, 220, 208, 77, 254, 26, 236, 166, 208, 136, 192, 206, 231, 147, 241, 251, 202, 92, 80, 186, 61, 36, 242, 204, 51, 130, 144, 53, 31, 180, 5, 145, 206, 200, 162, 180, 154, 42, 60, 241, 249, 35, 211, 194, 209, 222, 58, 224, 254, 228, 151, 249, 75, 19, 210, 74, 143, 218, 50, 94, 242, 97, 52, 117, 102, 62, 125, 41, 129, 172, 159, 26, 224, 231, 238, 22, 34, 144, 30, 35, 206, 135, 116, 3, 71, 227, 232, 189, 209, 105, 194, 62, 67, 1, 195, 249, 63, 232, 105, 29, 168, 109, 57, 182, 205, 174, 105, 105, 244, 220, 119, 46, 160, 18, 236, 232, 15, 148, 203, 189, 153, 48, 146, 104, 43, 108, 239, 246, 178, 138, 53, 124, 203, 191, 5, 200, 31, 77, 207, 4, 221, 255, 90, 213, 153, 71, 19, 90, 43, 161, 84, 50, 227, 139, 64, 236, 142, 245, 233, 92, 199, 242, 195, 77, 90, 193, 202, 223, 245, 4, 165, 180, 166, 254, 179, 14, 37, 17, 37, 64, 86, 55, 203, 40, 195, 93, 4, 114, 7, 163, 186, 177, 28, 230, 95, 246, 228, 197, 26, 234, 39, 168, 108, 187, 99, 78, 90, 109, 23, 244, 152, 241, 144, 51, 162, 202, 225, 144, 170, 45, 117, 89, 254, 62, 88, 39, 246, 232, 23, 90, 149, 226, 207, 214, 7, 113, 172, 14, 207, 225, 240, 121, 20, 136, 30, 168, 46, 127, 29, 97, 72, 13, 222, 241, 158, 93, 171, 140, 89, 120, 102, 134, 106, 115, 207, 82, 196, 186, 167, 32, 54, 30, 33, 148, 38, 215, 165, 220, 24, 112, 125, 209, 111, 105, 226, 39, 137, 198, 16, 217, 11, 165, 220, 62, 124, 182, 182, 244, 104, 121, 108, 103, 105, 9, 213, 201, 61, 182, 30, 35, 157, 75, 172, 162, 109, 245, 53, 204, 161, 32, 100, 134, 182, 124, 58, 215, 27, 172, 159, 186, 209, 204, 38, 171, 232, 238, 179, 106, 88, 54, 169, 44, 215, 96, 61, 40, 140, 236, 150, 109, 181, 205, 117, 84, 89, 242, 159, 178, 6, 136, 112, 51, 229, 113, 129, 84, 103, 205, 253, 217, 33, 244, 6, 171, 144, 180, 135, 94, 241, 150, 164, 200, 167, 91, 185, 27, 10, 156, 3, 80, 39, 84, 103, 67, 108, 142, 126, 191, 91, 248, 95, 14, 144, 156, 36, 223, 58, 239, 70, 212, 60, 184, 163, 251, 104, 187, 183, 230, 49, 254, 216, 210, 142, 144, 198, 208, 233, 153, 241, 181, 144, 237, 143, 37, 193, 172, 113, 211, 70, 193, 70, 139, 43, 11, 41, 31, 182, 59, 92, 214, 253, 124, 112, 110, 161, 202, 85, 36, 251, 99, 55, 200, 144, 241, 151, 79, 96, 167, 121, 157, 158, 138, 68, 157, 213, 46, 205, 103, 83, 14, 39, 123, 134, 49, 79, 78, 140, 72, 182, 48, 159, 105, 138, 75, 82, 178, 89, 110, 114, 185, 243, 15, 57, 173, 44, 245, 214, 55, 169, 32, 16, 218, 106, 119, 81, 124, 232, 223, 137, 171, 241, 161, 219, 19, 2, 74, 232, 225, 228, 130, 120, 160, 160, 11, 224, 127, 225, 35, 96, 117, 81, 99, 186, 139, 128, 84, 149, 203, 31, 26, 52, 77, 236, 201, 145, 117, 156, 218, 207, 230, 89, 155, 26, 76, 137, 141, 133, 117, 123, 41, 254, 93, 151, 9, 107, 193, 136, 85, 41, 235, 241, 135, 202, 205, 16, 78, 28, 146, 73, 189, 110, 128, 252, 230, 172, 93, 10, 185, 22, 242, 118, 128, 11, 63, 28, 21, 136, 133, 103, 140, 198, 54, 248, 25, 173, 53, 73, 197, 3, 208, 165, 149, 212, 162, 124, 181, 24, 70, 255, 133, 253, 195, 186, 5, 65, 139, 112, 78, 54, 97, 148, 61, 178, 251, 203, 56, 80, 164, 214, 45, 232, 53, 130, 247, 74, 189, 176, 122, 154, 168, 99, 130, 97, 79, 66, 84, 180, 108, 149, 133, 8, 234, 229, 128, 77, 30, 76, 104, 80, 194, 178, 36, 146, 236, 50, 176, 232, 178, 205, 18, 162, 180, 43, 128, 223, 15, 5, 136, 127, 190, 146, 207, 117, 127, 94, 61, 173, 2, 50, 66, 213, 177, 72, 202, 107, 214, 87, 50, 162, 218, 64, 29, 21, 214, 113, 22, 253, 254, 251, 222, 195, 144, 188, 39, 71, 164, 211, 35, 4, 134, 249, 188, 143, 127, 128, 13, 253, 168, 213, 243, 101, 156, 213, 195, 78, 95, 154, 112, 178, 144, 79, 235, 15, 127, 143, 254, 157, 100, 29, 17, 217, 53, 135, 197, 232, 205, 147, 206, 177, 29, 10, 129, 150, 84, 30, 151, 101, 40, 196, 155, 155, 201, 20, 109, 186, 147, 117, 159, 7, 158, 55, 129, 136, 146, 97, 71, 221, 139, 7, 98, 91, 144, 118, 103, 93, 60, 81, 13, 240, 173, 173, 19, 227, 116, 184, 73, 208, 234, 148, 15, 202, 92, 108, 178, 62, 217, 127, 154, 167, 206, 81, 210, 221, 74, 126, 168, 188, 52, 1, 238, 171, 191, 11, 201, 250, 165, 198, 137, 175, 207, 170, 172, 149, 203, 12, 246, 27, 99, 92, 185, 118, 94, 51, 87, 190, 185, 52, 149, 225, 226, 182, 162, 125, 60, 207, 17, 200, 97, 243, 64, 168, 201, 160, 66, 112, 107, 10, 124, 227, 43, 17, 156, 112, 138, 166, 176, 247, 207, 215, 236, 22, 33, 217, 213, 197, 201, 84, 117, 250, 26, 134, 168, 121, 56, 179, 30, 24, 233, 6, 218, 9, 28, 155, 250, 139, 239, 165, 143, 85, 35, 81, 249, 54, 179, 18, 112, 54, 15, 51, 254, 97, 242, 1, 218, 45, 85, 172, 88, 230, 236, 91, 147, 22, 43, 0, 138, 182, 67, 158, 49, 5, 65, 15, 137, 214, 231, 5, 94, 181, 187, 62, 205, 135, 186, 231, 192, 173, 190, 5, 144, 239, 50, 246, 83, 120, 212, 156, 213, 197, 140, 191, 32, 185, 112, 160, 247, 111, 160, 62, 102, 58, 40, 99, 210, 202, 68, 199, 131, 183, 131, 168, 205, 129, 148, 12, 77, 82, 53, 10, 42, 155, 97, 49, 144, 82, 68, 175, 220, 49, 223, 249, 138, 169, 247, 105, 32, 34, 252, 204, 140, 154, 212, 181, 168, 163, 150, 191, 60, 96, 154, 130, 203, 150, 69, 80, 221, 202, 106, 67, 189, 197, 242, 179, 41, 129, 147, 98, 189, 165, 199, 10, 12, 246, 124, 199, 8, 108, 14, 31, 224, 240, 40, 241, 12, 184, 97, 11, 9, 57, 35, 115, 7, 131, 60, 6, 80, 222, 18, 136, 49, 104, 153, 120, 89, 145, 226, 147, 5, 74, 102, 12, 216, 152, 223, 148, 219, 215, 209, 174, 95, 42, 224, 231, 199, 29, 40, 101, 184, 227, 81, 251, 90, 3, 165, 184, 203, 246, 92, 6, 61, 25, 14, 41, 44, 14, 192, 208, 226, 144, 121, 94, 106, 63, 54, 82, 242, 88, 78, 158, 121, 142, 115, 170, 119, 186, 29, 83, 217, 189, 40, 50, 230, 206, 127, 164, 130, 10, 226, 214, 128, 81, 6, 106, 145, 142, 180, 29, 139, 153, 135, 231, 61, 245, 70, 247, 86, 140, 114, 104, 229, 173, 178, 7, 85, 20, 196, 136, 244, 241, 113, 107, 2, 234, 26, 133, 133, 18, 155, 19, 3, 21, 1, 49, 179, 179, 190, 243, 16, 93, 22, 115, 73, 255, 231, 252, 188, 142, 182, 241, 209, 25, 187, 67, 18, 170, 229, 220, 57, 198, 186, 205, 222, 249, 36, 100, 9, 129, 243, 45, 218, 148, 4, 222, 70, 128, 11, 182, 59, 192, 12, 65, 215, 161, 227, 246, 253, 87, 11, 78, 71, 147, 112, 167, 171, 193, 15, 50, 115, 13, 166, 75, 255, 52, 3, 239, 208, 178, 184, 67, 210, 24, 175, 203, 136, 175, 136, 122, 30, 165, 229, 156, 111, 72, 43, 132, 177, 7, 227, 225, 125, 89, 56, 63, 171, 10, 243, 241, 33, 120, 167, 215, 106, 100, 32, 45, 180, 100, 51, 139, 162, 154, 123, 252, 19, 20, 8, 167, 43, 194, 225, 184, 21, 234, 170, 203, 23, 62, 50, 209, 179, 9, 187, 139, 84, 229, 230, 175, 135, 134, 25, 51, 64, 246, 235, 206, 84, 91, 9, 98, 53, 216, 241, 119, 79, 59, 92, 178, 182, 151, 171, 23, 67, 122, 238, 42, 131, 188, 216, 9, 242, 167, 134, 209, 35, 22, 103, 31, 51, 80, 132, 6, 255, 214, 251, 16, 215, 179, 50, 220, 152, 188, 201, 165, 130, 200, 189, 144, 58, 70, 85, 224, 241, 177, 61, 102, 105, 132, 251, 75, 2, 52, 238, 112, 247, 25, 56, 47, 192, 112, 32, 82, 160, 189, 225, 16, 219, 107, 95, 207, 76, 207, 159, 189, 213, 121, 23, 254, 231, 62, 78, 31, 72, 39, 32, 166, 22, 164, 241, 204, 221, 157, 185, 75, 58, 72, 122, 152, 243, 203, 200, 193, 175, 93, 249, 81, 138, 87, 165, 88, 231, 165, 35, 170, 138, 207, 253, 30, 176, 27, 138, 125, 172, 80, 167, 108, 128, 41, 102, 48, 204, 152, 104, 181, 27, 3, 229, 64, 211, 14, 55, 163, 60, 252, 245, 5, 20, 200, 116, 90, 250, 101, 156, 176, 168, 21, 244, 204, 78, 194, 66, 21, 27, 25, 43, 85, 226, 110, 165, 34, 116, 237, 135, 211, 158, 137, 136, 131, 245, 224, 111, 252, 112, 244, 229, 32, 30, 176, 16, 91, 196, 93, 13, 211, 244, 247, 102, 2, 53, 231, 73, 89, 184, 131, 158, 114, 85, 176, 49, 72, 49, 251, 162, 204, 38, 242, 6, 96, 144, 22, 118, 144, 103, 159, 206, 177, 239, 62, 111, 37, 238, 190, 217, 160, 171, 75, 164, 184, 25, 182, 55, 74, 202, 26, 58, 56, 240, 101, 121, 244, 0, 12, 135, 91, 74, 231, 201, 194, 220, 68, 11, 21, 161, 120, 86, 0, 75, 31, 124, 223, 96, 255, 97, 45, 148, 183, 53, 65, 232, 88, 205, 203, 88, 239, 243, 176, 37, 248, 196, 150, 206, 186, 204, 207, 210, 81, 200, 82, 77, 92, 113, 24, 10, 42, 221, 95, 164, 71, 106, 141, 63, 238, 13, 196, 105, 44, 36, 208, 248, 110, 120, 80, 203, 27, 23, 23, 210, 246, 114, 197, 171, 22, 124, 238, 103, 173, 209, 138, 134, 217, 93, 162, 254, 105, 121, 74, 49, 157, 2, 200, 67, 250, 91, 140, 1, 64, 97, 155, 125, 165, 48, 60, 152, 157, 199, 110, 215, 3, 44, 170, 183, 85, 182, 204, 2, 210, 216, 152, 239, 229, 51, 137, 98, 83, 179, 199, 71, 0, 65, 170, 180, 146, 242, 38, 220, 40, 159, 163, 102, 14, 15, 80, 222, 48, 82, 112, 118, 49, 157, 86, 98, 128, 73, 162, 206, 55, 183, 140, 238, 59, 48, 249, 100, 161, 127, 93, 94, 13, 84, 250, 215, 191, 95, 80, 17, 165, 19, 61, 31, 205, 154, 29, 107, 179, 42, 10, 63, 136, 10, 247, 223, 187, 49, 140, 218, 103, 76, 19, 144, 124, 170, 155, 174, 103, 187, 251, 26, 220, 180, 54, 252, 86, 133, 50, 122, 231, 12, 178, 143, 25, 130, 55, 38, 187, 115, 187, 192, 60, 50, 8, 205, 10, 145, 114, 122, 137, 67, 123, 89, 209, 235, 35, 80, 225, 67, 2, 169, 184, 108, 19, 86, 163, 109, 182, 26, 33, 182, 108, 144, 105, 198, 155, 116, 237, 160, 207, 120, 109, 48, 171, 94, 10, 13, 233, 33, 199, 10, 13, 214, 20, 58, 120, 230, 112, 159, 78, 27, 112, 192, 122, 123, 32, 61, 192, 201, 63, 63, 67, 79, 78, 218, 55, 122, 192, 47, 38, 70, 75, 153, 176, 184, 155, 135, 131, 248, 185, 71, 235, 94, 9, 66, 9, 107, 28, 52, 5, 198, 250, 110, 211, 235, 30, 200, 186, 170, 76, 45, 72, 60, 2, 32, 180, 110, 245, 243, 217, 229, 159, 209, 56, 224, 146, 254, 74, 133, 179, 111, 67, 205, 65, 37, 79, 1, 167, 89, 189, 146, 215, 94, 248, 186, 55, 191, 154, 247, 5, 178, 193, 138, 7, 33, 95, 137, 223, 58, 194, 206, 2, 212, 148, 96, 24, 170, 195, 40, 152, 161, 190, 150, 104, 150, 206, 60, 94, 236, 122, 43, 32, 112, 171, 176, 135, 74, 186, 189, 195, 213, 145, 131, 120, 194, 99, 165, 176, 254, 156, 32, 201, 148, 179, 126, 119, 65, 15, 96, 32, 94, 89, 3, 178, 198, 98, 144, 239, 119, 184, 202, 19, 215, 206, 169, 154, 182, 238, 233, 165, 12, 249, 119, 145, 59, 213, 6, 68, 113, 50, 144, 114, 144, 144, 238, 121, 49, 211, 10, 40, 252, 89, 201, 64, 210, 125, 200, 53, 24, 245, 45, 158, 27, 242, 143, 138, 110, 63, 170, 243, 253, 208, 29, 61, 230, 203, 210, 109, 67, 105, 132, 68, 31, 86, 162, 174, 62, 218, 223, 216, 248, 227, 121, 223, 58, 185, 96, 184, 207, 11, 177, 44, 33, 202, 10, 11, 162, 55, 141, 228, 182, 35, 178, 197, 229, 73, 53, 181, 8, 61, 104, 183, 73, 150, 13, 19, 175, 70, 158, 71, 140, 94, 75, 27, 80, 77, 32, 87, 20, 24, 238, 116, 70, 198, 203, 96, 118, 34, 30, 138, 145, 104, 111, 9, 47, 170, 143, 39, 12, 37, 102, 244, 99, 68, 234, 59, 35, 146, 7, 2, 188, 51, 217, 5, 143, 21, 89, 114, 188, 17, 114, 97, 216, 52, 112, 82, 154, 46, 84, 207, 36, 160, 13, 197, 17, 7, 39, 48, 121, 227, 243, 94, 134, 17, 155, 105, 126, 123, 83, 2, 41, 108, 199, 87, 212, 2, 54, 73, 93, 119, 215, 170, 161, 90, 68, 204, 128, 133, 39, 81, 214, 119, 222, 173, 255, 207, 152, 4, 203, 66, 247, 165, 0, 17, 103, 89, 212, 26, 196, 69, 191, 170, 21, 63, 162, 42, 102, 111, 95, 250, 25, 55, 206, 119, 255, 23, 217, 179, 183, 240, 18, 64, 41, 85, 75, 99, 125, 192, 56, 149, 54, 186, 201, 230, 64, 191, 128, 220, 116, 253, 68, 197, 115, 129, 158, 200, 92, 207, 140, 15, 33, 138, 104, 225, 244, 198, 40, 72, 164, 24, 120, 226, 87, 81, 252, 79, 140, 159, 202, 129, 13, 95, 97, 21, 206, 146, 72, 16, 206, 5, 217, 222, 55, 11, 132, 60, 123, 30, 37, 25, 29, 26, 105, 162, 219, 220, 99, 149, 110, 19, 163, 91, 246, 224, 216, 14, 69, 85, 249, 137, 214, 80, 91, 191, 5, 130, 132, 207, 169, 38, 180, 168, 147, 131, 208, 5, 30, 238, 206, 233, 9, 169, 124, 11, 160, 129, 88, 103, 86, 80, 242, 146, 238, 195, 21, 236, 238, 68, 251, 175, 159, 81, 19, 116, 192, 198, 51, 78, 10, 170, 59, 32, 114, 158, 210, 156, 191, 151, 19, 33, 156, 183, 14, 200, 94, 199, 121, 250, 61, 215, 25, 160, 140, 21, 112, 122, 89, 7, 229, 190, 218, 107, 231, 199, 107, 30, 246, 9, 147, 155, 171, 23, 86, 136, 32, 137, 123, 233, 198, 159, 50, 217, 248, 111, 118, 5, 216, 72, 22, 119, 249, 215, 22, 18, 201, 225, 230, 15, 71, 233, 166, 55, 157, 221, 173, 43, 36, 214, 90, 170, 90, 131, 227, 252, 179, 74, 57, 253, 146, 225, 1, 77, 119, 23, 16, 66, 230, 193, 138, 147, 95, 179, 74, 83, 140, 163, 151, 51, 100, 109, 59, 103, 28, 27, 35, 13, 148, 228, 227, 145, 180, 154, 195, 240, 16, 239, 196, 244, 249, 238, 81, 36, 169, 171, 159, 242, 181, 103, 179, 90, 96, 244, 198, 202, 186, 28, 27, 33, 71, 204, 73, 0, 72, 121, 13, 105, 49, 92, 205, 253, 99, 42, 142, 25, 101, 223, 48, 67, 35, 11, 138, 7, 46, 202, 240, 41, 74, 32, 135, 116, 59, 239, 123, 180, 9, 35, 233, 165, 241, 57, 235, 188, 29, 72, 54, 69, 51, 137, 8, 103, 238, 137, 244, 60, 8, 122, 157, 75, 101, 91, 60, 105, 72, 147, 224, 40, 253, 189, 111, 136, 223, 152, 48, 117, 235, 165, 14, 25, 47, 163, 27, 65, 15, 200, 131, 68, 84, 123, 47, 162, 175, 18, 125, 41, 125, 218, 151, 226, 126, 109, 255, 64, 102, 203, 133, 156, 58, 85, 191, 255, 98, 26, 65, 101, 166, 53, 217, 49, 194, 189, 38, 170, 38, 65, 106, 170, 13, 18, 1, 52, 181, 227, 131, 172, 71, 98, 45, 44, 195, 249, 141, 194, 204, 79, 70, 129, 157, 235, 217, 1, 50, 223, 100, 166, 26, 20, 64, 246, 94, 202, 9, 85, 77, 109, 218, 222, 41, 151, 211, 170, 54, 233, 167, 11, 2, 15, 138, 41, 46, 117, 141, 60, 155, 82, 184, 192, 227, 140, 33, 227, 159, 189, 157, 247, 200, 101, 193, 137, 236, 234, 191, 233, 190, 56, 13, 157, 245, 65, 92, 41, 83, 88, 43, 31, 125, 153, 8, 75, 139, 200, 244, 181, 85, 96, 90, 82, 5, 11, 169, 83, 61, 9, 70, 2, 202, 33, 188, 202, 128, 67, 134, 56, 41, 149, 11, 128, 121, 101, 129, 184, 183, 181, 142, 86, 53, 253, 88, 209, 245, 57, 6, 247, 87, 0, 199, 217, 86, 73, 149, 86, 137, 255, 37, 207, 3, 217, 212, 182, 83, 181, 115, 55, 39, 53, 41, 252, 194, 119, 127, 71, 253, 177, 235, 216, 144, 166, 67, 195, 205, 17, 169, 130, 80, 70, 112, 226, 207, 101, 231, 251, 239, 14, 59, 54, 22, 89, 220, 36, 129, 180, 11, 163, 16, 212, 244, 61, 89, 73, 240, 97, 239, 8, 138, 32, 6, 167, 229, 61, 244, 211, 48, 196, 79, 109, 138, 23, 112, 104, 178, 245, 32, 232, 230, 197, 171, 85, 128, 30, 71, 196, 160, 83, 88, 149, 247, 114, 152, 225, 184, 219, 158, 0, 204, 105, 16, 65, 136, 1, 100, 221, 136, 83, 178, 115, 101, 166, 118, 177, 8, 209, 189, 48, 156, 244, 188, 250, 239, 153, 12, 236, 66, 61, 171, 153, 126, 92, 219, 210, 46, 7, 117, 4, 24, 30, 206, 226, 215, 187, 242, 9, 104, 136, 193, 35, 91, 178, 129, 61, 64, 78, 124, 39, 221, 131, 136, 2, 200, 170, 50, 68, 203, 28, 162, 70, 4, 179, 80, 5, 49, 113, 208, 90, 82, 157, 229, 162, 237, 73, 95, 29, 61, 212, 193, 242, 144, 219, 166, 252, 57, 26, 77, 187, 21, 229, 73, 30, 22, 95, 158, 249, 73, 185, 89, 51, 193, 141, 75, 214, 72, 127, 232, 12, 119, 6, 208, 7, 89, 166, 180, 47, 226, 230, 83, 241, 175, 130, 106, 144, 118, 211, 164, 58, 176, 98, 144, 180, 61, 207, 243, 106, 178, 61, 158, 3, 109, 78, 179, 252, 161, 26, 93, 145, 103, 13, 92, 35, 89, 169, 38, 118, 30, 79, 134, 139, 80, 191, 161, 112, 55, 135, 252, 42, 113, 210, 6, 205, 159, 121, 186, 41, 211, 242, 55, 21, 11, 63, 167, 149, 22, 254, 173, 248, 154, 34, 57, 206, 128, 44, 248, 238, 107, 194, 26, 8, 36, 61, 55, 196, 247, 142, 194, 110, 23, 174, 159, 66, 141, 218, 155, 16, 51, 250, 193, 41, 174, 244, 203, 55, 177, 226, 200, 59, 199, 209, 32, 253, 216, 63, 171, 127, 244, 21, 91, 98, 255, 137, 205, 140, 23, 67, 106, 123, 131, 216, 146, 21, 109, 83, 254, 229, 181, 223, 111, 219, 139, 60, 191, 62, 118, 77, 167, 70, 144, 34, 233, 130, 245, 134, 155, 42, 134, 43, 56, 124, 193, 114, 176, 146, 81, 4, 158, 121, 219, 227, 19, 144, 182, 226, 80, 140, 31, 58, 106, 73, 62, 136, 108, 156, 124, 93, 176, 140, 167, 232, 188, 56, 205, 181, 185, 205, 66, 68, 109, 16, 245, 176, 116, 139, 223, 116, 112, 31, 253, 28, 145, 31, 255, 139, 39, 126, 18, 76, 0, 65, 203, 40, 37, 74, 158, 161, 243, 235, 143, 157, 215, 158, 31, 50, 242, 22, 139, 72, 118, 161, 166, 243, 105, 136, 108, 247, 33, 64, 22, 69, 93, 98, 181, 100, 112, 0, 52, 153, 146, 191, 82, 45, 8, 244, 9, 94, 32, 230, 65, 12, 239, 22, 56, 88, 252, 95, 115, 154, 75, 144, 239, 20, 34, 82, 72, 125, 51, 9, 14, 179, 193, 143, 82, 105, 33, 234, 13, 105, 199, 1, 94, 162, 108, 29, 108, 63, 250, 219, 218, 240, 72, 204, 202, 109, 65, 141, 85, 107, 191, 255, 241, 32, 82, 160, 35, 148, 27, 101, 85, 16, 55, 142, 234, 154, 205, 224, 101, 122, 108, 30, 99, 67, 109, 41, 58, 247, 79, 202, 101, 246, 23, 141, 103, 116, 214, 71, 177, 69, 3, 139, 125, 34, 12, 119, 18, 182, 237, 192, 65, 159, 66, 158, 188, 221, 200, 156, 67, 194, 240, 4, 251, 96, 2, 122, 23, 69, 214, 210, 242, 29, 219, 19, 40, 45, 156, 187, 43, 246, 160, 41, 105, 30, 207, 10, 145, 129, 20, 140, 104, 92, 247, 20, 213, 236, 152, 148, 246, 31, 37, 25, 255, 79, 7, 96, 9, 2, 204, 50, 227, 72, 213, 238, 50, 39, 15, 222, 50, 39, 64, 106, 117, 33, 254, 42, 64, 100, 103, 74, 177, 38, 169, 13, 225, 90, 97, 188, 127, 165, 227, 236, 4, 210, 4, 15, 205, 123, 231, 24, 61, 115, 244, 210, 57, 64, 152, 37, 174, 10, 102, 99, 179, 216, 204, 228, 159, 223, 48, 213, 242, 105, 113, 162, 183, 95, 142, 204, 171, 169, 121, 102, 4, 209, 99, 175, 129, 96, 30, 210, 200, 130, 192, 12, 80, 128, 64, 35, 176, 209, 151, 239, 144, 232, 222, 235, 89, 186, 29, 145, 219, 181, 77, 59, 191, 213, 255, 128, 124, 4, 162, 39, 229, 131, 138, 9, 58, 31, 83, 8, 137, 183, 184, 118, 228, 187, 157, 70, 101, 7, 253, 25, 212, 200, 73, 85, 134, 3, 159, 183, 229, 49, 45, 184, 165, 9, 131, 135, 243, 143, 37, 225, 84, 35, 21, 41, 170, 17, 251, 219, 7, 144, 70, 124, 52, 247, 130, 203, 84, 173, 83, 53, 115, 185, 254, 68, 219, 94, 25, 247, 36, 9, 145, 183, 32, 44, 15, 109, 195, 156, 78, 103, 237, 214, 196, 194, 107, 27, 147, 167, 113, 129, 67, 206, 212, 189, 155, 238, 70, 71, 113, 144, 246, 72, 209, 11, 77, 78, 129, 233, 35, 195, 128, 78, 17, 102, 37, 144, 118, 179, 245, 17, 128, 176, 174, 91, 240, 79, 141, 37, 95, 32, 212, 23, 120, 125, 217, 131, 78, 74, 254, 208, 210, 71, 16, 248, 122, 240, 139, 41, 26, 24, 16, 6, 22, 13, 59, 84, 81, 234, 243, 169, 53, 57, 244, 199, 133, 170, 2, 79, 203, 161, 94, 27, 234, 160, 213, 246, 188, 53, 226, 105, 16, 164, 235, 232, 5, 57, 190, 239, 240, 32, 128, 211, 130, 92, 188, 217, 45, 198, 20, 196, 62, 149, 217, 118, 211, 113, 66, 164, 100, 2, 132, 112, 81, 163, 200, 166, 205, 94, 66, 9, 214, 153, 221, 240, 95, 109, 86, 144, 251, 68, 145, 131, 107, 32, 248, 247, 141, 184, 15, 199, 243, 101, 246, 181, 136, 69, 44, 215, 240, 19, 112, 128, 173, 90, 212, 194, 156, 7, 16, 69, 80, 158, 124, 2, 194, 13, 249, 59, 137, 188, 59, 173, 8, 209, 162, 237, 107, 210, 130, 243, 91, 138, 154, 27, 190, 113, 160, 15, 146, 8, 54, 117, 236, 211, 81, 158, 189, 216, 99, 170, 70, 137, 126, 238, 137, 133, 116, 195, 18, 237, 65, 114, 251, 104, 31, 141, 200, 134, 199, 108, 143, 223, 231, 204, 254, 172, 152, 17, 164, 65, 133, 75, 239, 0, 202, 114, 173, 182, 1, 105, 225, 93, 90, 36, 145, 131, 71, 43, 63, 113, 233, 255, 221, 183, 255, 130, 218, 123, 76, 149, 247, 170, 202, 107, 107, 155, 16, 40, 92, 171, 8, 16, 218, 94, 135, 60, 77, 193, 136, 90, 222, 247, 54, 92, 234, 218, 138, 59, 129, 72, 139, 198, 164, 128, 117, 148, 171, 98, 179, 234, 119, 85, 124, 37, 167, 98, 85, 235, 200, 246, 160, 177, 127, 160, 186, 45, 55, 43, 74, 84, 199, 103, 88, 216, 124, 220, 11, 86, 31, 59, 170, 28, 27, 94, 47, 2, 200, 35, 114, 198, 142, 252, 53, 152, 101, 49, 171, 6, 64, 227, 232, 85, 65, 75, 174, 196, 95, 210, 149, 182, 95, 224, 91, 140, 109, 180, 233, 27, 151, 183, 150, 67, 164, 156, 93, 30, 66, 156, 159, 125, 13, 53, 217, 228, 248, 172, 212, 157, 23, 96, 32, 1, 52, 234, 249, 208, 169, 46, 182, 47, 0, 29, 246, 90, 51, 107, 67, 195, 86, 128, 238, 210, 35, 251, 184, 17, 1, 115, 159, 54, 242, 145, 181, 192, 162, 235, 3, 154, 60, 99, 93, 230, 173, 185, 17, 230, 22, 159, 34, 95, 151, 94, 8, 32, 133, 17, 199, 63, 113, 200, 48, 249, 140, 175, 48, 67, 80, 29, 14, 204, 139, 80, 57, 14, 150, 98, 245, 184, 118, 233, 141, 83, 143, 179, 71, 148, 231, 2, 253, 227, 125, 79, 141, 219, 7, 203, 147, 67, 32, 226, 244, 212, 229, 89, 44, 219, 224, 24, 49, 101, 186, 48, 38, 228, 168, 206, 130, 78, 167, 144, 201, 13, 154, 169, 10, 160, 27, 226, 190, 125, 230, 2, 176, 95, 153, 221, 56, 90, 3, 2, 143, 52, 245, 114, 137, 119, 103, 174, 239, 140, 178, 59, 75, 99, 79, 57, 162, 171, 245, 65, 247, 214, 236, 237, 28, 142, 208, 152, 30, 225, 132, 138, 49, 91, 143, 47, 166, 152, 237, 156, 109, 254, 17, 233, 2, 86, 247, 239, 48, 60, 42, 235, 249, 115, 26, 73, 67, 109, 81, 247, 117, 251, 35, 180, 193, 105, 145, 126, 112, 179, 157, 93, 94, 23, 17, 13, 253, 41, 66, 46, 63, 238, 219, 43, 19, 161, 132, 50, 172, 189, 61, 194, 145, 214, 26, 148, 231, 62, 63, 38, 247, 5, 43, 236, 160, 179, 98, 179, 143, 76, 49, 242, 1, 5, 198, 107, 67, 95, 46, 116, 248, 30, 108, 206, 153, 129, 168, 208, 173, 139, 25, 168, 70, 109, 81, 211, 214, 187, 110, 44, 79, 192, 2, 103, 254, 6, 112, 175, 174, 11, 118, 196, 50, 142, 187, 122, 165, 207, 246, 166, 133, 37, 53, 51, 55, 51, 59, 224, 132, 198, 13, 154, 56, 28, 255, 159, 94, 217, 171, 243, 173, 117, 210, 5, 75, 150, 28, 88, 144, 138, 94, 72, 52, 217, 3, 226, 120, 253, 104, 170, 5, 103, 224, 220, 179, 93, 119, 244, 80, 194, 150, 122, 201, 213, 190, 92, 47, 142, 74, 165, 194, 20, 74, 195, 55, 151, 149, 172, 177, 17, 201, 150, 131, 247, 62, 75, 19, 67, 180, 108, 141, 107, 221, 12, 89, 111, 15, 202, 78, 123, 214, 156, 20, 175, 66, 32, 241, 96, 252, 205, 184, 122, 115, 9, 192, 155, 98, 156, 201, 213, 182, 217, 17, 197, 244, 181, 214, 228, 7, 132, 122, 116, 80, 141, 146, 17, 77, 247, 60, 249, 215, 187, 79, 15, 200, 83, 131, 100, 70, 153, 175, 87, 162, 9, 190, 22, 128, 20, 18, 222, 154, 176, 172, 40, 218, 234, 18, 87, 74, 172, 51, 188, 244, 175, 71, 235, 30, 159, 73, 110, 94, 180, 31, 5, 130, 234, 174, 187, 116, 19, 146, 249, 132, 252, 62, 139, 252, 173, 115, 229, 215, 127, 71, 198, 170, 100, 148, 246, 78, 231, 116, 46, 199, 39, 14, 226, 6, 3, 57, 126, 40, 208, 232, 8, 183, 196, 166, 15, 172, 166, 127, 243, 80, 63, 236, 202, 96, 182, 224, 236, 221, 153, 21, 206, 245, 205, 0, 90, 77, 21, 221, 193, 234, 199, 173, 252, 254, 204, 128, 129, 85, 217, 39, 75, 158, 158, 22, 168, 233, 212, 98, 110, 170, 10, 221, 39, 248, 85, 200, 57, 243, 24, 162, 68, 198, 240, 160, 224, 50, 215, 21, 29, 232, 51, 232, 213, 5, 217, 60, 135, 128, 86, 46, 229, 191, 77, 151, 122, 229, 109, 32, 99, 105, 255, 102, 90, 154, 233, 147, 231, 226, 254, 48, 234, 130, 65, 109, 73, 224, 21, 139, 86, 150, 52, 141, 43, 61, 95, 91, 116, 74, 116, 44, 196, 71, 81, 185, 43, 161, 51, 219, 27, 6, 182, 51, 172, 36, 161, 58, 14, 235, 104, 27, 42, 95, 18, 83, 174, 166, 70, 229, 160, 249, 246, 92, 181, 254, 168, 0, 33, 30, 219, 104, 216, 1, 200, 56, 183, 206, 205, 178, 191, 18, 170, 184, 26, 162, 248, 100, 228, 82, 124, 90, 255, 118, 14, 167, 41, 100, 126, 127, 248, 232, 43, 232, 46, 5, 10, 174, 135, 138, 74, 122, 4, 150, 6, 223, 150, 22, 78, 141, 185, 35, 74, 254, 6, 244, 123, 231, 124, 220, 218, 47, 30, 249, 255, 102, 72, 204, 124, 115, 34, 202, 197, 11, 155, 121, 19, 126, 250, 143, 216, 213, 101, 171, 69, 43, 243, 246, 182, 69, 66, 255, 123, 194, 150, 193, 1, 226, 213, 123, 51, 196, 81, 89, 243, 84, 72, 29, 148, 164, 192, 113, 61, 76, 240, 176, 233, 228, 172, 92, 101, 149, 73, 128, 136, 112, 76, 73, 101, 206, 34, 60, 6, 193, 208, 154, 172, 187, 165, 128, 186, 51, 120, 180, 121, 205, 191, 103, 160, 197, 15, 205, 101, 181, 71, 94, 172, 235, 117, 182, 209, 24, 195, 190, 20, 9, 254, 213, 18, 172, 247, 62, 199, 231, 122, 239, 212, 69, 90, 205, 234, 237, 186, 175, 175, 179, 188, 29, 13, 61, 249, 142, 89, 142, 167, 169, 9, 139, 63, 252, 225, 10, 205, 117, 60, 71, 193, 144, 158, 4, 202, 90, 119, 68, 224, 253, 243, 110, 189, 153, 193, 186, 249, 123, 42, 180, 182, 229, 163, 21, 57, 198, 147, 115, 168, 185, 55, 22, 56, 100, 23, 244, 196, 99, 18, 171, 108, 167, 250, 97, 34, 32, 86, 46, 227, 229, 211, 22, 180, 209, 19, 223, 121, 28, 64, 157, 17, 0, 237, 82, 65, 202, 187, 44, 83, 99, 186, 187, 123, 153, 39, 207, 225, 230, 250, 45, 95, 3, 216, 3, 58, 207, 181, 150, 67, 212, 237, 33, 130, 142, 159, 204, 124, 70, 174, 183, 45, 131, 238, 36, 13, 220, 162, 193, 38, 150, 108, 183, 162, 227, 39, 67, 133, 194, 19, 213, 131, 19, 215, 54, 108, 47, 159, 73, 138, 14, 125, 170, 144, 150, 57, 223, 10, 119, 173, 255, 95, 251, 231, 228, 160, 205, 119, 243, 105, 26, 157, 68, 255, 149, 240, 79, 58, 51, 214, 140, 171, 253, 11, 34, 208, 223, 95, 145, 68, 118, 159, 72, 179, 109, 39, 141, 120, 14, 164, 112, 124, 159, 254, 105, 189, 68, 84, 13, 246, 195, 11, 124, 100, 166, 191, 126, 14, 35, 42, 143, 0, 132, 134, 1, 99, 39, 25, 161, 172, 75, 112, 74, 18, 52, 20, 18, 192, 183, 237, 113, 14, 134, 232, 245, 166, 12, 83, 169, 230, 198, 158, 4, 31, 203, 24, 231, 92, 229, 182, 44, 85, 180, 202, 192, 238, 229, 49, 59, 88, 9, 238, 31, 91, 189, 149, 253, 116, 52, 10, 134, 43, 17, 86, 176, 82, 79, 133, 64, 245, 68, 125, 213, 119, 242, 141, 239, 155, 29, 191, 2, 9, 94, 98, 93, 2, 167, 105, 218, 143, 48, 25, 234, 147, 184, 193, 10, 166, 255, 154, 101, 58, 172, 86, 211, 209, 243, 134, 207, 62, 115, 243, 190, 110, 238, 216, 62, 219, 169, 250, 91, 23, 22, 184, 21, 13, 91, 64, 77, 163, 103, 6, 143, 57, 141, 181, 194, 20, 7, 33, 50, 61, 111, 172, 29, 130, 121, 6, 85, 161, 20, 164, 233, 6, 166, 101, 162, 219, 126, 236, 83, 1, 42, 182, 188, 148, 230, 56, 204, 124, 221, 118, 15, 213, 182, 123, 162, 150, 253, 42, 25, 71, 240, 196, 138, 50, 104, 241, 252, 202, 136, 12, 145, 182, 80, 209, 69, 205, 210, 243, 86, 165, 243, 47, 236, 78, 111, 97, 45, 208, 27, 138, 160, 240, 66, 251, 92, 172, 38, 98, 114, 108, 88, 105, 18, 75, 134, 131, 245, 104, 146, 146, 54, 77, 237, 79, 70, 250, 179, 136, 92, 63, 113, 105, 178, 8, 253, 176, 154, 105, 245, 234, 202, 183, 180, 147, 218, 94, 139, 78, 247, 167, 71, 187, 252, 253, 208, 160, 102, 47, 149, 40, 250, 105, 94, 92, 101, 59, 121, 46, 175, 8, 46, 85, 232, 146, 148, 247, 253, 102, 20, 39, 35, 195, 98, 133, 138, 135, 192, 104, 41, 4, 81, 85, 235, 64, 164, 79, 129, 68, 109, 201, 47, 134, 79, 9, 1, 112, 126, 135, 12, 72, 103, 18, 117, 27, 151, 64, 244, 18, 54, 88, 30, 253, 241, 191, 59, 38, 253, 255, 67, 107, 214, 48, 148, 195, 153, 18, 161, 102, 219, 117, 168, 37, 63, 58, 20, 111, 160, 163, 234, 199, 149, 137, 167, 189, 110, 37, 189, 173, 152, 217, 166, 203, 35, 81, 47, 40, 157, 33, 173, 41, 118, 108, 185, 91, 173, 213, 141, 35, 54, 138, 39, 233, 155, 251, 4, 23, 137, 247, 97, 129, 27, 140, 63, 64, 247, 13, 60, 53, 243, 18, 172, 74, 168, 105, 51, 14, 204, 33, 195, 57, 73, 180, 192, 119, 147, 251, 168, 25, 218, 2, 140, 232, 165, 63, 242, 148, 12, 228, 94, 15, 194, 28, 9, 49, 220, 227, 129, 139, 80, 104, 87, 60, 192, 165, 107, 63, 40, 213, 25, 226, 131, 2, 65, 244, 197, 187, 152, 164, 199, 66, 59, 131, 70, 208, 49, 225, 154, 116, 35, 55, 34, 190, 75, 147, 163, 239, 12, 244, 138, 60, 202, 20, 169, 254, 229, 181, 40, 229, 173, 46, 210, 202, 216, 215, 186, 59, 184, 249, 8, 90, 255, 150, 2, 11, 155, 119, 241, 116, 230, 222, 177, 238, 43, 3, 85, 44, 88, 201, 165, 204, 82, 165, 127, 197, 187, 3, 212, 122, 172, 50, 33, 195, 126, 100, 138, 174, 164, 151, 151, 203, 76, 68, 133, 149, 139, 174, 19, 251, 4, 107, 182, 111, 229, 118, 26, 71, 173, 114, 217, 134, 154, 156, 81, 131, 56, 189, 130, 33, 12, 148, 249, 200, 54, 183, 119, 210, 114, 175, 52, 197, 14, 138, 118, 26, 134, 126, 174, 80, 197, 24, 121, 26, 113, 70, 216, 212, 94, 233, 22, 232, 59, 104, 243, 121, 133, 51, 145, 35, 170, 243, 57, 45, 126, 50, 11, 53, 76, 153, 76, 62, 188, 147, 40, 139, 246, 196, 243, 56, 252, 233, 65, 215, 154, 80, 253, 183, 74, 2, 143, 237, 66, 28, 250, 77, 21, 119, 91, 221, 20, 50, 150, 73, 208, 23, 220, 148, 163, 99, 68, 129, 66, 242, 170, 109, 178, 186, 34, 13, 160, 224, 45, 155, 186, 16, 32, 234, 201, 169, 160, 154, 11, 129, 33, 92, 180, 164, 79, 100, 61, 37, 44, 53, 112, 69, 146, 211, 236, 121, 68, 151, 247, 168, 158, 0, 201, 62, 2, 29, 84, 65, 6, 144, 117, 181, 68, 168, 54, 90, 255, 118, 200, 156, 52, 129, 148, 74, 226, 135, 240, 62, 181, 133, 82, 92, 177, 95, 246, 70, 100, 248, 134, 225, 59, 140, 20, 55, 99, 210, 178, 250, 103, 60, 225, 238, 240, 214, 37, 44, 176, 82, 75, 122, 30, 161, 204, 53, 69, 93, 25, 134, 59, 28, 68, 36, 75, 94, 233, 236, 74, 94, 107, 51, 137, 237, 150, 166, 146, 227, 105, 249, 17, 194, 14, 15, 146, 250, 114, 216, 96, 173, 163, 198, 0, 188, 172, 214, 80, 155, 110, 185, 69, 223, 25, 218, 186, 240, 51, 253, 201, 30, 108, 183, 117, 100, 84, 59, 129, 253, 237, 245, 239, 187, 226, 123, 85, 128, 182, 227, 127, 79, 113, 60, 68, 149, 53, 171, 123, 62, 169, 187, 113, 239, 75, 12, 146, 67, 84, 57, 144, 236, 209, 123, 192, 232, 228, 232, 218, 229, 164, 173, 4, 35, 166, 198, 187, 49, 78, 186, 77, 97, 153, 111, 216, 30, 180, 46, 192, 222, 189, 213, 86, 185, 50, 246, 12, 22, 45, 213, 150, 90, 61, 62, 173, 219, 192, 219, 65, 160, 205, 135, 63, 122, 156, 166, 255, 64, 94, 85, 39, 143, 244, 105, 111, 92, 107, 170, 34, 87, 228, 14, 191, 74, 124, 163, 92, 212, 154, 214, 43, 158, 186, 168, 173, 145, 62, 3, 176, 141, 105, 61, 211, 1, 55, 227, 215, 139, 153, 179, 251, 122, 35, 168, 1, 8, 233, 133, 190, 196, 163, 155, 136, 142, 83, 171, 107, 155, 11, 84, 170, 134, 160, 202, 52, 58, 90, 90, 11, 198, 207, 182, 165, 112, 103, 170, 19, 20, 103, 171, 87, 253, 166, 150, 87, 102, 117, 196, 202, 74, 101, 192, 78, 16, 54, 210, 220, 115, 243, 222, 239, 75, 252, 27, 84, 108, 221, 37, 17, 222, 82, 251, 29, 227, 253, 14, 172, 168, 163, 57, 26, 249, 38, 2, 173, 163, 16, 170, 60, 161, 238, 28, 51, 234, 21, 96, 242, 46, 191, 133, 148, 186, 90, 123, 172, 228, 175, 164, 99, 32, 103, 10, 151, 234, 84, 102, 171, 117, 254, 207, 146, 197, 71, 96, 52, 125, 130, 39, 65, 13, 38, 10, 145, 201, 54, 65, 196, 177, 123, 144, 188, 250, 139, 116, 78, 129, 26, 3, 5, 3, 13, 248, 186, 116, 5, 35, 209, 123, 30, 54, 142, 194, 3, 148, 204, 4, 69, 66, 95, 206, 106, 251, 185, 18, 221, 121, 218, 94, 148, 122, 152, 209, 204, 203, 237, 18, 59, 169, 97, 18, 72, 143, 148, 215, 160, 82, 91, 191, 33, 32, 124, 107, 190, 92, 183, 92, 30, 123, 57, 163, 223, 16, 239, 103, 43, 90, 141, 98, 85, 93, 141, 218, 254, 116, 39, 163, 12, 123, 251, 46, 8, 161, 141, 250, 149, 69, 143, 159, 77, 117, 100, 142, 55, 14, 155, 89, 163, 150, 109, 166, 72, 208, 127, 118, 189, 171, 207, 163, 181, 0, 174, 167, 14, 6, 14, 168, 161, 75, 145, 164, 162, 158, 193, 187, 239, 213, 219, 251, 250, 121, 122, 108, 191, 62, 108, 116, 24, 70, 96, 79, 40, 62, 115, 103, 7, 250, 243, 128, 104, 60, 171, 11, 254, 62, 218, 63, 173, 250, 110, 168, 107, 93, 139, 242, 80, 254, 235, 191, 239, 106, 172, 140, 74, 51, 252, 115, 176, 31, 56, 28, 255, 24, 146, 86, 50, 24, 192, 125, 92, 40, 201, 118, 167, 208, 46, 96, 64, 21, 228, 194, 48, 98, 224, 217, 147, 111, 76, 58, 255, 139, 172, 8, 186, 255, 247, 36, 196, 228, 146, 210, 9, 32, 154, 233, 106, 215, 41, 131, 55, 193, 178, 104, 49, 219, 224, 238, 36, 176, 180, 78, 201, 238, 193, 244, 193, 222, 133, 201, 0, 141, 78, 34, 87, 183, 163, 62, 85, 115, 194, 182, 156, 177, 167, 77, 71, 201, 252, 191, 142, 150, 194, 75, 195, 217, 2, 152, 148, 89, 167, 55, 201, 194, 220, 244, 151, 231, 205, 96, 147, 76, 155, 48, 206, 103, 218, 68, 13, 56, 38, 223, 145, 32, 99, 180, 55, 252, 249, 38, 239, 137, 32, 185, 244, 127, 142, 120, 245, 214, 233, 157, 178, 196, 199, 42, 8, 202, 184, 53, 126, 108, 65, 68, 152, 37, 61, 33, 59, 124, 150, 99, 142, 203, 22, 240, 140, 87, 88, 121, 80, 45, 157, 143, 104, 174, 245, 153, 35, 239, 194, 158, 57, 255, 163, 205, 94, 114, 171, 29, 176, 192, 93, 177, 108, 106, 32, 9, 220, 96, 144, 34, 106, 252, 204, 170, 21, 251, 53, 197, 221, 154, 252, 21, 41, 60, 103, 241, 191, 13, 47, 235, 76, 128, 139, 76, 245, 245, 194, 183, 106, 62, 214, 242, 16, 158, 129, 66, 194, 237, 25, 161, 148, 97, 246, 209, 27, 145, 25, 249, 195, 88, 76, 209, 249, 214, 40, 158, 160, 251, 227, 180, 62, 116, 143, 182, 93, 13, 142, 0, 171, 241, 199, 12, 161, 109, 61, 152, 136, 144, 165, 141, 130, 153, 51, 37, 135, 237, 228, 114, 191, 244, 218, 250, 48, 102, 205, 118, 252, 235, 8, 184, 18, 107, 60, 36, 249, 5, 131, 162, 94, 138, 108, 12, 125, 171, 144, 111, 139, 121, 23, 130, 57, 24, 185, 180, 141, 166, 192, 8, 59, 220, 235, 31, 226, 39, 140, 142, 121, 173, 124, 54, 61, 187, 38, 250, 149, 76, 198, 209, 67, 126, 101, 162, 238, 151, 77, 41, 141, 9, 104, 188, 237, 19, 210, 221, 33, 242, 138, 198, 181, 46, 247, 109, 125, 134, 158, 77, 246, 12, 104, 189, 159, 52, 97, 164, 94, 72, 72, 194, 72, 55, 104, 235, 113, 79, 169, 90, 182, 255, 245, 221, 115, 68, 190, 75, 187, 122, 150, 181, 236, 223, 138, 223, 14, 30, 234, 155, 182, 133, 152, 168, 34, 139, 132, 254, 195, 32, 52, 201, 9, 199, 180, 164, 25, 22, 153, 45, 164, 175, 143, 49, 155, 150, 158, 111, 130, 201, 58, 48, 86, 37, 192, 208, 116, 146, 53, 251, 187, 54, 135, 69, 105, 25, 253, 168, 250, 175, 239, 104, 199, 66, 239, 52, 40, 38, 81, 230, 230, 206, 152, 6, 169, 72, 120, 120, 126, 177, 174, 182, 155, 214, 92, 111, 246, 152, 153, 123, 221, 12, 21, 239, 209, 56, 10, 144, 19, 255, 212, 40, 197, 141, 124, 64, 30, 214, 35, 56, 200, 65, 122, 143, 1, 33, 162, 252, 0, 204, 51, 220, 111, 36, 248, 30, 160, 101, 171, 60, 90, 218, 232, 70, 53, 180, 57, 179, 191, 54, 167, 239, 114, 212, 42, 14, 93, 196, 153, 20, 176, 61, 2, 239, 41, 14, 74, 190, 11, 56, 37, 177, 136, 156, 104, 45, 142, 153, 85, 148, 136, 42, 240, 140, 165, 191, 228, 228, 117, 139, 96, 243, 0, 203, 137, 37, 225, 252, 105, 80, 131, 91, 31, 5, 82, 111, 95, 59, 62, 117, 193, 36, 15, 109, 114, 167, 146, 137, 29, 252, 55, 47, 85, 119, 116, 255, 240, 66, 252, 105, 203, 21, 211, 233, 28, 89, 238, 151, 111, 183, 254, 4, 35, 144, 132, 100, 66, 182, 93, 239, 195, 38, 10, 220, 228, 124, 165, 1, 80, 140, 187, 116, 97, 154, 159, 133, 160, 38, 185, 7, 30, 238, 39, 121, 248, 69, 42, 18, 72, 129, 246, 148, 103, 244, 106, 86, 228, 213, 30, 136, 12, 195, 51, 50, 141, 147, 172, 122, 61, 3, 110, 61, 205, 228, 178, 147, 101, 6, 51, 118, 164, 153, 23, 230, 219, 15, 189, 26, 13, 236, 5, 213, 69, 188, 101, 2, 89, 88, 117, 155, 177, 26, 114, 198, 56, 159, 235, 187, 176, 56, 233, 48, 39, 233, 20, 235, 9, 131, 239, 171, 246, 202, 29, 91, 227, 9, 37, 55, 38, 33, 143, 40, 255, 216, 55, 199, 0, 104, 34, 174, 124, 77, 161, 171, 206, 51, 217, 67, 48, 191, 189, 213, 208, 112, 200, 253, 219, 234, 209, 81, 200, 1, 97, 190, 53, 177, 123, 88, 218, 236, 200, 207, 93, 143, 183, 172, 189, 98, 138, 151, 88, 129, 95, 186, 120, 161, 15, 202, 115, 168, 14, 200, 102, 110, 93, 49, 108, 172, 79, 138, 50, 1, 217, 147, 26, 74, 207, 241, 228, 150, 58, 132, 246, 18, 190, 51, 135, 235, 56, 168, 126, 249, 145, 39, 14, 152, 191, 26, 88, 162, 134, 160, 28, 180, 160, 99, 107, 12, 197, 82, 235, 77, 247, 182, 40, 217, 209, 250, 195, 215, 40, 106, 24, 13, 251, 129, 165, 113, 45, 10, 195, 18, 26, 140, 130, 141, 126, 189, 102, 65, 153, 176, 237, 44, 247, 53, 11, 102, 77, 54, 171, 105, 40, 202, 23, 50, 78, 152, 3, 55, 108, 83, 232, 31, 151, 166, 153, 92, 209, 168, 203, 186, 249, 231, 168, 242, 222, 127, 13, 244, 17, 199, 105, 232, 70, 85, 37, 13, 152, 195, 80, 35, 217, 163, 19, 194, 255, 39, 116, 180, 13, 220, 246, 106, 40, 205, 228, 68, 167, 221, 178, 191, 102, 153, 241, 98, 219, 117, 214, 77, 17, 178, 127, 206, 59, 219, 245, 251, 42, 155, 101, 59, 109, 111, 214, 60, 212, 184, 211, 82, 57, 179, 248, 75, 177, 3, 254, 17, 105, 1, 35, 197, 209, 85, 132, 112, 153, 37, 141, 60, 219, 100, 106, 105, 44, 225, 90, 165, 14, 96, 20, 1, 70, 6, 250, 136, 79, 107, 141, 69, 183, 129, 178, 240, 228, 112, 28, 55, 10, 243, 231, 152, 89, 85, 157, 41, 124, 30, 141, 21, 86, 215, 255, 25, 29, 166, 176, 194, 170, 203, 247, 25, 59, 214, 85, 146, 216, 234, 130, 171, 2, 116, 192, 9, 189, 196, 20, 8, 179, 8, 2, 55, 21, 75, 29, 195, 209, 36, 16, 143, 104, 221, 30, 172, 174, 50, 204, 177, 82, 231, 0, 192, 2, 199, 117, 6, 35, 222, 189, 234, 90, 219, 5, 226, 56, 147, 215, 223, 187, 223, 7, 89, 238, 169, 38, 96, 124, 187, 238, 246, 113, 133, 79, 133, 131, 217, 99, 181, 215, 14, 27, 222, 176, 24, 116, 148, 193, 57, 199, 161, 91, 194, 111, 19, 109, 215, 244, 41, 107, 70, 67, 14, 244, 124, 64, 24, 91, 68, 9, 37, 183, 58, 155, 192, 252, 76, 122, 168, 125, 158, 142, 73, 72, 158, 127, 52, 147, 95, 118, 208, 89, 79, 88, 105, 189, 190, 244, 43, 202, 250, 22, 212, 89, 1, 214, 61, 72, 247, 186, 69, 226, 187, 185, 145, 140, 60, 105, 243, 107, 87, 151, 197, 1, 169, 192, 153, 223, 213, 231, 30, 185, 152, 56, 217, 13, 167, 4, 82, 105, 74, 129, 211, 91, 23, 16, 179, 91, 115, 225, 115, 46, 0, 104, 100, 17, 102, 183, 60, 247, 4, 255, 27, 165, 193, 7, 19, 169, 100, 157, 102, 2, 167, 178, 120, 240, 37, 193, 142, 44, 166, 100, 156, 214, 196, 216, 74, 204, 113, 175, 2, 28, 230, 22, 9, 204, 69, 214, 11, 195, 194, 157, 167, 176, 91, 133, 132, 98, 71, 167, 186, 110, 45, 255, 122, 241, 251, 78, 193, 47, 199, 253, 139, 114, 237, 147, 130, 20, 143, 135, 18, 231, 24, 219, 91, 51, 127, 94, 123, 38, 188, 24, 93, 77, 140, 126, 48, 95, 13, 57, 156, 126, 8, 135, 41, 29, 66, 204, 2, 225, 97, 58, 55, 117, 62, 58, 200, 199, 195, 51, 230, 246, 251, 84, 62, 160, 64, 113, 171, 76, 167, 0, 77, 119, 129, 232, 31, 110, 165, 204, 254, 92, 73, 97, 230, 13, 36, 50, 61, 10, 236, 67, 45, 2, 190, 4, 197, 102, 82, 54, 15, 19, 68, 206, 141, 230, 146, 192, 179, 203, 110, 63, 161, 23, 199, 192, 57, 202, 2, 242, 95, 124, 219, 135, 158, 177, 138, 182, 1, 209, 152, 111, 84, 206, 5, 111, 114, 190, 215, 101, 54, 55, 122, 179, 156, 63, 27, 243, 8, 176, 176, 200, 119, 229, 190, 68, 219, 202, 70, 118, 193, 177, 108, 55, 47, 105, 200, 116, 74, 240, 201, 110, 207, 70, 223, 235, 27, 157, 122, 86, 247, 25, 237, 217, 127, 123, 235, 142, 184, 149, 56, 120, 225, 203, 203, 54, 5, 221, 128, 101, 103, 90, 222, 11, 246, 138, 51, 39, 107, 195, 235, 104, 113, 126, 43, 159, 251, 193, 219, 224, 13, 102, 116, 249, 105, 34, 140, 238, 94, 162, 194, 202, 45, 191, 65, 227, 31, 47, 27, 153, 116, 241, 139, 15, 222, 212, 106, 251, 161, 255, 179, 169, 75, 82, 242, 106, 242, 128, 144, 104, 170, 60, 186, 136, 141, 25, 198, 110, 52, 216, 241, 209, 25, 178, 244, 118, 153, 248, 69, 146, 101, 76, 28, 235, 92, 225, 9, 88, 151, 40, 244, 255, 3, 243, 238, 118, 12, 121, 201, 57, 49, 231, 238, 11, 81, 198, 180, 147, 134, 69, 237, 63, 63, 24, 236, 194, 212, 107, 185, 13, 56, 152, 126, 96, 92, 95, 149, 234, 207, 187, 205, 77, 105, 83, 246, 138, 49, 196, 71, 149, 233, 164, 88, 83, 78, 247, 199, 123, 63, 202, 96, 84, 205, 87, 188, 18, 221, 17, 27, 178, 228, 247, 108, 172, 30, 247, 252, 5, 95, 246, 33, 134, 166, 77, 152, 140, 243, 168, 109, 62, 133, 7, 37, 228, 2, 197, 212, 84, 118, 5, 104, 203, 183, 122, 147, 42, 134, 198, 208, 183, 177, 73, 155, 145, 175, 134, 57, 18, 29, 221, 213, 155, 41, 108, 209, 37, 208, 156, 0, 155, 210, 203, 185, 45, 90, 205, 27, 10, 140, 198, 107, 191, 183, 189, 114, 118, 174, 57, 254, 12, 182, 72, 101, 144, 128, 67, 234, 74, 209, 196, 125, 41, 31, 10, 142, 52, 187, 194, 157, 217, 179, 255, 0, 199, 110, 171, 36, 149, 123, 133, 130, 71, 121, 197, 234, 125, 125, 46, 200, 163, 111, 205, 10, 168, 46, 15, 45, 38, 183, 221, 240, 153, 252, 33, 220, 53, 152, 254, 111, 77, 239, 183, 113, 148, 216, 109, 195, 154, 91, 30, 40, 239, 27, 254, 16, 148, 241, 33, 105, 186, 125, 242, 201, 213, 90, 89, 10, 91, 54, 136, 65, 72, 34, 6, 23, 82, 250, 71, 50, 210, 25, 159, 238, 13, 77, 182, 222, 77, 174, 103, 174, 62, 62, 192, 112, 123, 35, 209, 152, 35, 57, 54, 166, 201, 48, 199, 182, 175, 84, 7, 40, 97, 123, 109, 12, 119, 170, 4, 109, 21, 165, 200, 41, 80, 183, 133, 166, 184, 106, 61, 93, 49, 184, 91, 140, 71, 106, 120, 8, 24, 253, 128, 135, 240, 227, 195, 124, 68, 91, 95, 229, 156, 85, 64, 58, 40, 205, 81, 203, 48, 180, 91, 94, 145, 166, 228, 106, 48, 176, 117, 73, 11, 10, 166, 138, 163, 114, 120, 120, 77, 60, 92, 255, 249, 228, 101, 213, 57, 158, 211, 174, 80, 29, 135, 205, 150, 63, 37, 139, 206, 214, 159, 52, 25, 34, 120, 227, 116, 123, 134, 31, 132, 156, 12, 251, 46, 5, 23, 198, 99, 21, 15, 172, 28, 170, 120, 132, 235, 233, 162, 137, 132, 76, 184, 207, 249, 57, 42, 193, 73, 209, 163, 220, 22, 216, 214, 91, 253, 94, 238, 115, 241, 104, 42, 92, 222, 152, 17, 185, 135, 116, 15, 230, 145, 242, 37, 250, 237, 143, 25, 28, 217, 207, 220, 57, 180, 29, 12, 245, 112, 181, 99, 193, 51, 167, 134, 88, 86, 242, 185, 82, 148, 59, 122, 148, 212, 185, 120, 201, 215, 130, 191, 39, 81, 123, 250, 251, 154, 61, 71, 34, 56, 196, 39, 167, 176, 10, 249, 49, 73, 218, 128, 243, 89, 138, 187, 165, 56, 96, 41, 237, 57, 56, 176, 235, 114, 147, 237, 92, 62, 139, 78, 151, 154, 228, 164, 123, 118, 39, 31, 217, 186, 64, 230, 220, 162, 199, 110, 145, 248, 74, 195, 59, 129, 153, 181, 62, 206, 218, 112, 144, 131, 50, 157, 173, 214, 235, 0, 4, 77, 76, 223, 225, 83, 139, 234, 11, 158, 33, 57, 170, 15, 90, 185, 251, 88, 142, 16, 196, 129, 138, 186, 55, 226, 117, 71, 178, 86, 88, 190, 1, 6, 97, 11, 239, 96, 115, 161, 192, 107, 204, 178, 69, 243, 151, 85, 140, 140, 45, 39, 97, 235, 26, 6, 16, 55, 128, 215, 140, 208, 134, 12, 248, 216, 16, 220, 80, 207, 194, 109, 131, 240, 44, 145, 254, 1, 146, 81, 214, 70, 39, 26, 39, 151, 252, 195, 97, 10, 62, 46, 115, 27, 234, 149, 167, 12, 184, 50, 70, 64, 172, 3, 120, 101, 30, 138, 198, 196, 234, 4, 187, 19, 36, 142, 118, 75, 64, 234, 247, 17, 80, 142, 37, 73, 13, 47, 158, 51, 75, 96, 194, 93, 139, 205, 245, 247, 133, 208, 83, 128, 182, 232, 197, 162, 9, 27, 76, 15, 3, 57, 227, 217, 195, 17, 167, 70, 125, 193, 98, 46, 75, 16, 59, 122, 247, 81, 54, 166, 129, 105, 71, 135, 242, 40, 17, 241, 250, 182, 213, 5, 6, 241, 233, 110, 16, 75, 96, 96, 132, 93, 170, 24, 180, 126]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 8       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            s = tf.floor(tf.linspace(0.0, M, batch_size, name=\"linspace\"))\n",
        "            \n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            plt.plot(t, triangle3, 'o')\n",
        "            plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            ad_noise_std = ad_noise_std = tf.linspace(0.0, 40.0, M, name=\"linspace\")\n",
        "            noise = {}\n",
        "            for i in range(M):\n",
        "              i = tf.cast(i, tf.int32)\n",
        "              noise[i] = tf.random_normal([batch_size], mean=0.0, stddev=ad_noise_std[i]) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            sess = tf.Session()\n",
        "            #print(sess.run((s_hat)))\n",
        "            wt = list([1,2,3,4,5,6,7,8])\n",
        "            wt = tf.convert_to_tensor(wt, dtype=tf.float32)\n",
        "            #s_hat = (tf.math.multiply(s_hat,wt))\n",
        "            \n",
        "            print(sess.run((wt)))\n",
        "            #print(sess.run((s_hat)))\n",
        "            \n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "                \n",
        "            # Performance metrics\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size,lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size,input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size,input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr,iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size,lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "YeHghWVRf7hO",
        "outputId": "aeeeda59-f078-47d9-8063-2259971dcdfe"
      },
      "source": [
        "train_EbNodB = 40\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "#lr = (tf.train.exponential_decay(1e-10, global_step = 100, decay_steps=100, decay_rate=1.30))\n",
        "#lr = tf.cast(lr, tf.float32)\n",
        "#lr = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
        "#lr  = [0.005, 0.007, 0.009, 0.011, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040, 0.045, 0.05]\n",
        "lr = 0.01\n",
        "#epoch = [1, 10, 100, 500, 1000, 2000, 5000, 10000, 50000, 100000, 500000]\n",
        "\n",
        "epoch = [10000]\n",
        "# decay_learning_rate = learning_rate *decay_rate ^ (global_step / decay_steps)\n",
        "#global_step = tf.train.get_global_step()\n",
        "#tf.summary.scalar('learning_rate', lr) \n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [10000 , 0.01, 10000]\n",
        "#    [10000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, 1000],\n",
        "    [10000, 1000],\n",
        "    [10000, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3952\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3953\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3954\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aa6cd6a31e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel_file_uw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/ae_k_{}_n_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_uw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-936c6b976d6c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, k, n, seed, filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstellations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-936c6b976d6c>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#plt.plot(t, s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m#s = tf.convert_to_tensor(s, dtype=tf.int64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-936c6b976d6c>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;34m'''The transmitter'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_var_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = (8,8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_var_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         result = _clip(\n\u001b[0;32m--> 135\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3954\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3955\u001b[0m     return gen_array_ops.gather_v2(\n\u001b[0;32m-> 3956\u001b[0;31m         params, indices, axis, name=name)\n\u001b[0m\u001b[1;32m   3957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   4080\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4081\u001b[0m         \u001b[0;34m\"GatherV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4082\u001b[0;31m                     batch_dims=batch_dims, name=name)\n\u001b[0m\u001b[1;32m   4083\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    630\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    631\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GA7ihjf7hZ"
      },
      "source": [
        "#model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "#ae = AE(k,n,seed)\n",
        "#ae.train(training_params, validation_params)\n",
        "#ae.save(model_file); # Save the trained autoencoder if you want to reuse it later\n",
        "#sess = tf.Session()\n",
        "#print(sess.run((rmse_uw_0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            plt.plot(t, triangle3, 'o')\n",
        "            plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            ad_noise_std = ad_noise_std = tf.linspace(0.0, 40.0, M, name=\"linspace\")\n",
        "            noise = {}\n",
        "            for i in range(M):\n",
        "              noise[i] = tf.random_normal([batch_size], mean=0.0, stddev=ad_noise_std[i]) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            sess = tf.Session()\n",
        "            #print(sess.run((s_hat)))\n",
        "            wt = list([1,2,3,4,5,6,7,8])\n",
        "            wt = tf.convert_to_tensor(wt, dtype=tf.float32)\n",
        "            #s_hat = (tf.math.multiply(s_hat,wt))\n",
        "            \n",
        "            print(sess.run((wt)))\n",
        "            #print(sess.run((s_hat)))\n",
        "            \n",
        "            #for i in wt:\n",
        "               #s_hat(i) = i*s_hat(i)\n",
        "            print('See here')\n",
        "            print(s_hat)\n",
        "            print('See above')\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**2)\n",
        "\n",
        "                \n",
        "            # Performance metrics\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjh_J3MIlyrF"
      },
      "source": [
        "train_EbNodB = 40\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "#lr = (tf.train.exponential_decay(1e-10, global_step = 100, decay_steps=100, decay_rate=1.30))\n",
        "#lr = tf.cast(lr, tf.float32)\n",
        "#lr = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
        "#lr  = [0.005, 0.007, 0.009, 0.011, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040, 0.045, 0.05]\n",
        "lr = 0.01\n",
        "#epoch = [1, 10, 100, 500, 1000, 2000, 5000, 10000, 50000, 100000, 500000]\n",
        "\n",
        "epoch = [10000]\n",
        "# decay_learning_rate = learning_rate *decay_rate ^ (global_step / decay_steps)\n",
        "#global_step = tf.train.get_global_step()\n",
        "#tf.summary.scalar('learning_rate', lr) \n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [10000 , 0.01, 10000]\n",
        "#    [50000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, 1000],\n",
        "    [10000, 1000],\n",
        "    [10000, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        #weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
        "        weights = np.arange(M)\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            #plt.xlim(-2,2)\n",
        "            #plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "#plot_constellation_2(ae,range(0,ae.M))\n",
        "plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), tr)\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), tr)\n",
        "#print(np.shape(tr))\n",
        "#print(np.shape(tr_hat))\n",
        "print(tr)\n",
        "print(tr_hat_w)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rmse_uw = {}\n",
        "for j in range(M):\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "rmse_w = {}\n",
        "for j in range(M):\n",
        "  rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "\n",
        "\n",
        "#message_factor = [15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1]\n",
        "message_factor = np.flip(np.arange(M))\n",
        "message_factor[M-1] = 1\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "    \n",
        "#rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7, rmse_uw_8, rmse_uw_9, rmse_uw_10, rmse_uw_11, rmse_uw_12, rmse_uw_13, rmse_uw_14, rmse_uw_15]\n",
        "rmse_uw = (np.divide(rmse_uwA,message_factor))\n",
        "#rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7, rmse_w_8, rmse_w_9, rmse_w_10, rmse_w_11, rmse_w_12, rmse_w_13, rmse_w_14, rmse_w_15]\n",
        "rmse_w = (np.divide(rmse_wA,message_factor))\n",
        "#message = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "message = np.arange(M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE deviation log2')\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])\n",
        "\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "\n",
        "print(np.sum(rmse_uwA))\n",
        "print(np.sum(rmse_wA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i"
      },
      "source": [
        "ae.plot_constellation();\n",
        "#plot_constellation_2(ae,range(0,ae.M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75eUVLFnq1JO"
      },
      "source": [
        "ae_Weighted.plot_constellation();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        "np.divide(10,2)\n",
        "#IEEE 802.11p Max TX Power 30dBm\n",
        "#Path loss model corresponding: 20log10(d(m))  (We take n=2, FSPL)\n",
        "#0.1W corresponds to 20dBm or -10dB\n",
        "#7dB corresponds to 5W\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.850–5.925 GHz).\n",
        "# Noise Power -100dBm, TX Power is 20dBm and that \\\n",
        "#256m- -94dBm, 128m- -88dBm, 1m"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}